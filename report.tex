
\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{array}

\begin{document}

\begin{titlepage}
    \centering
    \vspace*{2cm}
    {\Huge\bfseries Report\par}
    \vspace{2cm}
    {\Large\itshape Homework 2\par}
    \vspace{0.5cm}
    {\large\itshape Goal: Image Classification with CNN for a self driving car in OpenAI Gym\par}
    \vfill
    {\Large Andrea Massignan\par}
    {\Large 1796802\par}
    \vfill
    {\large\today\par}
\end{titlepage}

\section{Introduction}

In this homework assignment, the objective is to address the image classification problem focused on understanding the behavior of a racing car within the OpenAI Gym environment [1]. The task involves classifying 96x96 color images, each corresponding to one of the five distinct actions available for controlling the car.
\newline
\newline
The provided dataset is segregated into training and test sets, with each image accompanied by a label indicating the specific action associated with the car's control. The dataset structure is organized into folders, where each folder is labeled with the identifier of the corresponding action.
\newline
\newline
The primary goal of this assignment is to develop an image classification model capable of accurately predicting the action taken by the racing car based on the visual input. This entails training a model on the labeled training set and evaluating its performance on the separate test set.
\newline
\newline
The classification task is significant in understanding and predicting the decision-making process of an autonomous agent navigating a simulated racing environment. Effective solutions to this problem can have applications in various domains, such as autonomous vehicle control and reinforcement learning.

\section{Methodology}

The image classification task is addressed using a convolutional neural network (CNN) model. The CNN architecture is implemented using the Keras API [2] with a TensorFlow backend [3]. The model is trained on the labeled training set and evaluated on the separate test set.
\newline
\newline
Here is how I approached the problem.

\section{Data Preprocessing}

For the training set, data augmentation is performed to increase the model's robustness and improve its generalization capabilities. The ImageDataGenerator is configured with the following augmentation parameters:

\begin{itemize}
    \item \textbf{Rescaling:} Normalizes pixel values to the range [0, 1] by dividing each pixel value by 255.
    \item \textbf{Zoom Range:} Randomly applies zooming with a range of 0.1 to introduce variations in image scale.
    \item \textbf{Width Shift Range:} Randomly shifts the width of the image by a fraction of 0.1 to introduce horizontal variations.
    \item \textbf{Height Shift Range:} Randomly shifts the height of the image by a fraction of 0.1 to introduce vertical variations.
    \item \textbf{Horizontal Flip:} Randomly flips images horizontally to diversify training samples.
    \item \textbf{Vertical Flip:} No vertical flipping is applied to maintain the orientation of the racing car images.
\end{itemize}

The augmented training data is generated in batches using the \texttt{flow\_from\_directory} method of the ImageDataGenerator. The generator is configured to read images from the 'trainingset' directory, resize them to a target size of (96, 96), use RGB color mode, generate batches of size 64, apply one-hot encoding for categorical labels, shuffle the data, and consider it as part of the training subset.

\subsection{Validation Data Preprocessing}

For the validation set, no data augmentation is applied to maintain the integrity of the original images. The ImageDataGenerator for validation is configured only with rescaling, ensuring consistency with the preprocessing applied during training.
\newline
\newline
The validation data generator is created using the \texttt{flow\_from\_directory} method, reading images from the 'validationset' directory, resizing them to (96, 96), and generating batches of size 64. The validation data is not shuffled, and class mode is set to 'categorical' for compatibility with the model.

\subsection{Dataset Information}

After configuring the generators, relevant information about the dataset is extracted, including the number of training samples, number of classes, and the input shape of the images. The class names are obtained from the generator's class indices. The extracted information is printed to the console for reference.

\section{Convolutional Neural Network (CNN) Architecture}

In this section, we present the architecture of the Convolutional Neural Network (CNN) designed for the image classification problem in the context of racing car behavior. The CNN model is implemented using Keras, consisting of multiple convolutional and dense layers to capture hierarchical features from the input images.

\subsection{Model Architecture}

The CNN model is constructed using the Sequential API in Keras, allowing the sequential stacking of layers. Let's break down each layer of the model:

\begin{enumerate}
    \item \textbf{C1 Convolutional Layer:} \\
          The first convolutional layer consists of 15 filters with a kernel size of (4, 4). Rectified Linear Unit (ReLU) activation function is applied to introduce non-linearity. This layer processes the input image to capture low-level features.

    \item \textbf{C2 Convolutional Layer:} \\
          The second convolutional layer comprises 20 filters with a kernel size of (4, 4). ReLU activation is applied, followed by a max-pooling layer with a pool size of (2, 2). This layer further extracts hierarchical features and reduces spatial dimensions through pooling.

    \item \textbf{C3 Convolutional Layer:} \\
          The third convolutional layer includes 30 filters with a kernel size of (4, 4). ReLU activation is applied, followed by another max-pooling layer. This layer captures higher-level features and reduces spatial dimensions.

    \item \textbf{Flatten Layer:} \\
          The flatten layer transforms the 3D tensor output from the convolutional layers into a 1D tensor, preparing it for the subsequent dense layers.

    \item \textbf{D1 Dense Layer:} \\
          The first dense layer consists of 128 neurons with ReLU activation. A dropout layer with a rate of 0.4 is introduced to prevent overfitting.

    \item \textbf{D2 Dense Layer:} \\
          The second dense layer comprises 96 neurons with ReLU activation. Another dropout layer with a rate of 0.4 is added.

    \item \textbf{Output Layer:} \\
          The output layer has neurons equal to the number of classes, with softmax activation to produce class probabilities.
          \\
          This layer is responsible for predicting the action label.

\end{enumerate}

\subsection{Model Compilation}

The model is compiled using the RMSprop optimizer with a learning rate of 0.0001. Categorical crossentropy is chosen as the loss function, as it is suitable for multi-class classification problems. The accuracy metric is used to monitor the model's performance during training.

\section{Training Method with Callbacks}

In this section, we outline the training methodology used to train the Convolutional Neural Network (CNN) for the racing car behavior classification problem. The training process incorporates specific callbacks to enhance the model's training efficiency and prevent overfitting.

\subsection{Callback Definitions}

Two crucial callbacks are employed during the training process:

\begin{enumerate}
    \item \textbf{Early Stopping Callback:} \\
          The \texttt{EarlyStopping} callback is configured to monitor the validation loss (\texttt{val\_loss}). If there is no improvement in validation loss for a consecutive number of epochs (patience), training is terminated early to prevent overfitting. The \texttt{restore\_best\_weights} parameter ensures that the model is restored to its best weights.

    \item \textbf{Reduce Learning Rate on Plateau Callback:} \\
          The \texttt{ReduceLROnPlateau} callback dynamically adjusts the learning rate during training. It monitors the validation loss and reduces the learning rate (\texttt{factor=0.2}) if there is no improvement for a certain number of epochs (patience). The minimum learning rate is set to \(1 \times 10^{-6}\).
\end{enumerate}

\subsection{Training Configuration}

The training process is executed within a \texttt{try-except} block to handle potential interruptions. The model is trained using the \texttt{fit} method, with the following parameters:

\begin{itemize}
    \item \texttt{train\_generator} and \texttt{validation\_generator:} Data generators for training and validation datasets, respectively.

    \item \texttt{epochs=50:} The total number of training epochs.

    \item \texttt{steps\_per\_epoch:} The number of steps (batches) to process from the training generator in one epoch.

    \item \texttt{validation\_data:} Validation generator for evaluating the model's performance during training.

    \item \texttt{validation\_steps:} The number of steps (batches) to process from the validation generator in one epoch.

    \item \texttt{callbacks:} A list containing the defined callbacks (\texttt{early\_stopping} and \texttt{reduce\_lr}).
\end{itemize}

\subsection{Handling Interrupts}

The \texttt{except KeyboardInterrupt} block handles potential manual interruptions, allowing for a graceful termination of the training process.

\section{Evaluation and Results Analysis}

In this section, we discuss the evaluation of the trained Convolutional Neural Network (CNN) model and analyze the obtained results.

\subsection{Evaluation Code}

The following code is responsible for evaluating the model on the validation dataset:

\begin{verbatim}
    val_steps = validation_generator.n//validation_generator.batch_size + 1
    loss, acc = model.evaluate_generator(validation_generator, steps=val_steps)
    print('Test loss: %f' % loss)
    print('Test accuracy: %f' % acc)
\end{verbatim}

\begin{itemize}
    \item \texttt{val\_steps:} The variable is calculated to determine the number of steps (batches) needed to cover the entire validation dataset. It ensures that all data is considered during evaluation.

    \item \texttt{loss, acc:} These variables store the computed loss and accuracy values, respectively, during the evaluation.

    \item \texttt{model.evaluate\_generator:} This method evaluates the model on the validation generator, considering the specified number of steps (\texttt{val\_steps}).

    \item \texttt{print('Test loss: \%f' \% loss):} Outputs the computed test loss.

    \item \texttt{print('Test accuracy: \%f' \% acc):} Outputs the computed test accuracy.

\end{itemize}

\subsection{Results Analysis}

The results of the evaluation are as follows:

\begin{itemize}
    \item \texttt{Test loss:} The computed loss on the validation dataset is 1.120741.

    \item \texttt{Test accuracy:} The computed accuracy on the validation dataset is 0.654784.

\end{itemize}

\subsection{Meaning of Results}

The test loss provides a quantitative measure of how well the model performs on the validation dataset.
\\
A lower loss indicates better performance. In this case, the test loss is 1.120741.
\\
\\
The test accuracy, representing the proportion of correctly classified samples, is 0.654784. This value indicates the percentage of correctly predicted labels among all validation samples.
\\
\\
Further analysis and potential improvements can be explored based on these results, such as fine-tuning the model architecture, adjusting hyperparameters, or augmenting the dataset.
\\
\\
A step we will try to is by running a second diverse iteration of the CNN.

\section{Classification Report Analysis}

In this section, we analyze the results of a classification report obtained from a model evaluation.

\subsection{Classification Report}

The following is the classification report:
\\
\begin{center}
\centering
\begin{tabular}{lcccccc}
\hline
& precision & recall & f1-score & support \\
\hline
0 & 0.500 & 0.075 & 0.131 & 53 \\
1 & 0.287 & 0.509 & 0.367 & 110 \\
2 & 0.417 & 0.512 & 0.460 & 162 \\
3 & 0.810 & 0.736 & 0.771 & 758 \\
4 & 0.000 & 0.000 & 0.000 & 15 \\
\hline
accuracy & - & - & 0.638 & 1098 \\
macro avg & 0.403 & 0.367 & 0.346 & 1098 \\
weighted avg & 0.674 & 0.638 & 0.643 & 1098 \\
\hline
\end{tabular}
\end{center}

\subsection{Interpretation}

The classification report provides a detailed overview of the performance of a classification model across different classes. Here is the interpretation of key metrics:

\begin{itemize}
    \item \texttt{precision:} Precision is the ratio of correctly predicted positive observations to the total predicted positives. It is highest for class 3 (0.810), indicating a high proportion of correctly predicted positive samples.

    \item \texttt{recall:} Recall, or sensitivity, is the ratio of correctly predicted positive observations to the all observations in actual class. Class 3 also has the highest recall (0.736), indicating the model captures a significant portion of actual positive samples for this class.

    \item \texttt{f1-score:} The F1-score is the weighted average of precision and recall. It is particularly useful when the class distribution is imbalanced. Class 3 has the highest F1-score (0.771), reflecting a balance between precision and recall.

    \item \texttt{support:} Support is the number of actual occurrences of the class in the specified dataset. Class 3 has the highest support (758), indicating a relatively large number of samples for this class.

    \item \texttt{accuracy:} Accuracy is the ratio of correctly predicted observations to the total observations. The overall accuracy is 0.638.

    \item \texttt{macro avg:} Macro-averaging computes the metric independently for each class and then takes the average. The macro-averaged precision, recall, and F1-score are 0.403, 0.367, and 0.346, respectively.

    \item \texttt{weighted avg:} Weighted averaging computes the metric for each class and then takes the weighted average based on the support. The weighted-averaged precision, recall, and F1-score are 0.674, 0.638, and 0.643, respectively.

\end{itemize}

\section{Analysis of Car Behavior}

In this section, we analyze the behavior of the car in the provided code that utilizes a trained model to control the car in the CarRacing-v2 environment.

\subsection{Observations}

The observed behavior of the car, is described below:

\begin{itemize}
    \item \textbf{Stability:} The car exhibits a certain degree of stability, as it manages to stay on the right track for the majority of the simulation.

    \item \textbf{Unstable Moments:} Despite overall semi-stability, there are instances where the car exhibits some instability. As sometimes it gets outside of the track, or spins to get back on the correct path.

\end{itemize}


\end{document}