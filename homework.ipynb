{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Iteration ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if GPU is available\n",
    "print('GPU is', 'available' if tf.config.list_physical_devices('GPU') else 'NOT AVAILABLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import numpy as np\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras import optimizers, callbacks\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from gymnasium.wrappers import RecordVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Preparation\n",
    "# Define the paths to your training and validation data\n",
    "trainingset = 'train/'\n",
    "validationset = 'test/'\n",
    "\n",
    "batch_size = 64\n",
    "target_size = (96, 96)  # Adjust based on your dataset\n",
    "\n",
    "# Training data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rotation_range=20,\n",
    "    shear_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation data should not be augmented\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=trainingset,\n",
    "    target_size=target_size,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    directory=validationset,\n",
    "    target_size=target_size,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Gather dataset information\n",
    "num_samples = train_generator.n\n",
    "num_classes = train_generator.num_classes\n",
    "input_shape = train_generator.image_shape\n",
    "\n",
    "classnames = list(train_generator.class_indices.keys())\n",
    "img_h, img_w, img_channels = input_shape\n",
    "print(f\"Image height = {img_h}, Image Width = {img_w}, Channels = {img_channels}\")\n",
    "print(f\"Image input shape: {input_shape}\")\n",
    "print(f\"Classes: {classnames}\")\n",
    "print(f\"Loaded {num_samples} training samples from {num_classes} classes.\")\n",
    "print(f\"Loaded {validation_generator.n} validation samples from {validation_generator.num_classes} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell X: Analyze Class Distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count of samples per class\n",
    "class_counts = train_generator.classes\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(class_names, [np.sum(class_counts == i) for i in range(len(class_names))], color='skyblue')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Create Separate Generators for Minority Classes\n",
    "import os\n",
    "\n",
    "# Define the target size and batch size\n",
    "target_size = (96, 96)\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize data augmentation for minority classes\n",
    "minority_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=30,\n",
    "    shear_range=0.2,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Paths to minority class directories\n",
    "minority_classes = ['0', '4']\n",
    "train_dir = 'train/'\n",
    "\n",
    "augmented_generators = []\n",
    "\n",
    "for cls in minority_classes:\n",
    "    class_dir = os.path.join(train_dir, cls)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        print(f\"Directory for class {cls} not found. Skipping augmentation.\")\n",
    "        continue\n",
    "    generator = minority_datagen.flow_from_directory(\n",
    "        directory=train_dir,\n",
    "        target_size=target_size,\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"categorical\",\n",
    "        classes=[cls],  # Only target the minority class\n",
    "        shuffle=True\n",
    "    )\n",
    "    augmented_generators.append(generator)\n",
    "\n",
    "print(f\"Number of augmented generators: {len(augmented_generators)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell Y: Compute Class Weights\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "\n",
    "# Convert to a dictionary\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"Class Weights:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Combine Original and Augmented Generators\n",
    "import itertools\n",
    "\n",
    "# Create a combined generator\n",
    "train_combined = itertools.chain(\n",
    "    train_generator,\n",
    "    *augmented_generators\n",
    ")\n",
    "\n",
    "# Define a generator that yields data from the combined generator\n",
    "def combined_generator(combined):\n",
    "    for data in combined:\n",
    "        yield data\n",
    "\n",
    "# Reset the iterator\n",
    "train_combined = combined_generator(train_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Deeper CNN with Different Activation\n",
    "def DeepCNN(input_shape, num_classes):\n",
    "    model = Sequential(name=\"DeepCNN\")\n",
    "\n",
    "    # First Convolutional Block\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # Flatten and Dense Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model with RMSprop optimizer\n",
    "    optimizer = optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the model\n",
    "model_deep = DeepCNN(input_shape, num_classes)\n",
    "model_deep.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "def ImprovedDeepCNN(input_shape, num_classes):\n",
    "    model = models.Sequential(name=\"ImprovedDeepCNN\")\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # Third Convolutional Block\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    \n",
    "    # Global Average Pooling and Dense Layers\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(512, kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile the model with Adam optimizer\n",
    "    optimizer = optimizers.Adam(learning_rate=0.0001)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the improved model\n",
    "model_improved = ImprovedDeepCNN(input_shape, num_classes)\n",
    "model_improved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Select Model\n",
    "# Choose one of the models defined above\n",
    "model = model_improved  # Replace with model_deep or model_improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Define Callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Increased patience for potentially longer training\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,  # Increased patience\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Optionally, add ModelCheckpoint to save the best model\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    'models/best_model.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Train the Model\n",
    "# Calculate steps per epoch and validation steps\n",
    "steps_per_epoch = int(np.ceil(train_generator.n / batch_size))\n",
    "val_steps = int(np.ceil(validation_generator.n / batch_size))\n",
    "\n",
    "try:\n",
    "    history = model.fit(\n",
    "        train_combined,\n",
    "        epochs=100,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "        class_weight=class_weights_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted.\")\n",
    "\n",
    "# Save the final model based if you chose deep or improved model\n",
    "if model == model_deep:\n",
    "    model.save('models/final_model_deep.keras')\n",
    "else:\n",
    "    model.save('models/final_model_improved.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Average\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_deep = load_model('models/final_model_deep.keras')\n",
    "model_improved = load_model('models/final_model_improved.keras')\n",
    "\n",
    "input_shape_deep = model_deep.input_shape[1:]  # Exclude batch size\n",
    "input_shape_improved = model_improved.input_shape[1:]\n",
    "\n",
    "assert input_shape_deep == input_shape_improved, \"Input shapes of both models must be the same.\"\n",
    "\n",
    "# Create an input layer that matches the input shape\n",
    "input_layer = Input(shape=input_shape_deep)\n",
    "\n",
    "# Get predictions from both models\n",
    "preds_deep = model_deep(input_layer)\n",
    "preds_improved = model_improved(input_layer)\n",
    "\n",
    "# Average the outputs\n",
    "averaged_preds = Average()([preds_deep, preds_improved])\n",
    "\n",
    "# Create the combined model\n",
    "combined_model = Model(inputs=input_layer, outputs=averaged_preds)\n",
    "\n",
    "# Compile the combined model\n",
    "combined_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Save the combined model\n",
    "combined_model.save('models/combined_model.keras')\n",
    "\n",
    "# Cell 11: Evaluate the Model\n",
    "# Load the combined model\n",
    "combined_model = load_model('models/combined_model.keras')\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_acc = combined_model.evaluate(validation_generator, verbose=1)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Evaluate the Model\n",
    "# Load the best saved model\n",
    "best_model = load_model('models/combined_model.keras')\n",
    "\n",
    "# Evaluate on validation data\n",
    "val_steps = int(np.ceil(validation_generator.n / batch_size))\n",
    "loss, acc = best_model.evaluate(validation_generator, steps=val_steps, verbose=1)\n",
    "print(f'Loss: {loss:.4f}')\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Classification Report\n",
    "preds = best_model.predict(validation_generator, steps=val_steps, verbose=0)\n",
    "Ypred = np.argmax(preds, axis=1)\n",
    "Ytest = validation_generator.classes  # Ensure shuffle=False in validation_generator\n",
    "\n",
    "print(classification_report(Ytest, Ypred, target_names=classnames, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Confusion Matrix\n",
    "cm = confusion_matrix(Ytest, Ypred)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=classnames, yticklabels=classnames, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Text-Based Confusion Matrix\n",
    "conf = []\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        if i != j and cm[i][j] > 0:\n",
    "            conf.append([i, j, cm[i][j]])\n",
    "\n",
    "conf = np.array(conf)\n",
    "conf = conf[np.argsort(-conf[:, 2])]  # Sort by descending error count\n",
    "\n",
    "print(f'{\"True\":<16} {\"Predicted\":<16} {\"Errors\":<10} {\"Error %\":<10}')\n",
    "print('-' * 60)\n",
    "for k in conf:\n",
    "    true_class = classnames[int(k[0])]\n",
    "    pred_class = classnames[int(k[1])]\n",
    "    errors = int(k[2])\n",
    "    error_pct = (errors / validation_generator.n) * 100\n",
    "    print(f'{true_class:<16} -> {pred_class:<16} {errors:<10} {error_pct:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Model Deployment with Gymnasium (Final Revised for Continuous Actions)\n",
    "import numpy as np\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "def play(env, model, predefined_actions):\n",
    "    seed = 2000\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "\n",
    "    # Drop initial frames with no action\n",
    "    no_action = predefined_actions[0]  # [0.0, 0.0, 0.0]\n",
    "    for _ in range(50):\n",
    "        obs, _, _, _, _ = env.step(no_action)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Preprocess the observation\n",
    "        img = preprocess_observation(obs, target_size)\n",
    "        p = model.predict(np.expand_dims(img, axis=0))  # Shape: (1, 5)\n",
    "        predicted_class = np.argmax(p)  # Integer 0-4\n",
    "\n",
    "        # Map the predicted class to a predefined action\n",
    "        action = predefined_actions.get(predicted_class, predefined_actions[0])  # Array\n",
    "\n",
    "        # Ensure the action is a float32 NumPy array\n",
    "        action = np.array(action, dtype=np.float32)\n",
    "\n",
    "        # Step the environment with the action\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    env.close()\n",
    "\n",
    "def preprocess_observation(obs, target_size):\n",
    "    from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
    "\n",
    "    # Convert observation to PIL Image\n",
    "    img = array_to_img(obs)\n",
    "    # Resize image\n",
    "    img = img.resize(target_size)\n",
    "    # Convert to array and normalize\n",
    "    img = img_to_array(img) / 255.0\n",
    "    return img\n",
    "\n",
    "# Define predefined actions (Continuous)\n",
    "predefined_actions = {\n",
    "    0: np.array([0.0, 0.0, 0.0], dtype=np.float32),  # No Action\n",
    "    1: np.array([-1.0, 0.0, 0.0], dtype=np.float32), # Steer Left\n",
    "    2: np.array([1.0, 0.0, 0.0], dtype=np.float32),  # Steer Right\n",
    "    3: np.array([0.0, 1.0, 0.0], dtype=np.float32),  # Accelerate (Gas)\n",
    "    4: np.array([0.0, 0.0, 1.0], dtype=np.float32),  # Brake\n",
    "    # Add more actions as needed\n",
    "}\n",
    "\n",
    "# Initialize the environment without 'continuous' parameter\n",
    "env_arguments = {\n",
    "    'domain_randomize': False,\n",
    "    'render_mode': 'rgb_array'\n",
    "}\n",
    "\n",
    "env_name = 'CarRacing-v3'\n",
    "env = gym.make(env_name, **env_arguments)\n",
    "\n",
    "# Wrap the environment to record videos\n",
    "video_dir = 'video_recordings'  # Specify the directory to save video recordings\n",
    "env = RecordVideo(env, video_dir)\n",
    "\n",
    "print(\"Environment:\", env_name)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "\n",
    "# Play the game using the trained model\n",
    "play(env, best_model, predefined_actions)\n",
    "#play(env, best_model_regression, predefined_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional Approach) Reinforcement Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything loaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, losses\n",
    "\n",
    "# Set TensorFlow to use channels_first format\n",
    "tf.keras.backend.set_image_data_format('channels_first')\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTIL FUNCTIONS\n",
    "def write_json_to_file(data, file_path):\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "\n",
    "def make_all_paths(is_dynamic_root=True, dir_name=\"rl_class\"):\n",
    "    ROOT = \"reinforcement_learning\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") if is_dynamic_root else \"\"\n",
    "    dir_name = f\"{dir_name}_{timestamp}\" if is_dynamic_root else dir_name\n",
    "    path_root = os.path.join(ROOT, dir_name)\n",
    "    for sub_dir in [\"models\", \"plots\", \"videos\"]:\n",
    "        os.makedirs(os.path.join(path_root, sub_dir), exist_ok=True)\n",
    "    return path_root + \"/\"\n",
    "\n",
    "\n",
    "def preprocess_frame_car(frame):\n",
    "    # Convert to grayscale, resize, and normalize\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = cv2.resize(frame, (96, 96)).astype(np.float32) / 255.0\n",
    "    return frame\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "class Config:\n",
    "    SEED = random.randint(0, 1000)\n",
    "\n",
    "    STARTING_EPISODE_TRAIN = 0\n",
    "    ENDING_EPISODE_TRAIN = STARTING_EPISODE_TRAIN + 50\n",
    "\n",
    "    STARTING_EPISODE_TEST = ENDING_EPISODE_TRAIN + 1\n",
    "    ENDING_EPISODE_TEST = STARTING_EPISODE_TEST + 50\n",
    "\n",
    "    SKIP_FRAMES = 2\n",
    "    TRAINING_BATCH_SIZE = 32\n",
    "    UPDATE_TARGET_MODEL_FREQUENCY = 5\n",
    "    N_FRAMES = 3\n",
    "    HIDDEN_DIMENSION_FC = 150\n",
    "\n",
    "    GAS_WEIGHT = 1.3\n",
    "\n",
    "    ACTION_SPACE = [\n",
    "        (-1, 1, 0.2), (0, 1, 0.2), (1, 1, 0.2),\n",
    "        (-1, 1, 0), (0, 1, 0), (1, 1, 0),\n",
    "        (-1, 0, 0.2), (0, 0, 0.2), (1, 0, 0.2),\n",
    "        (-1, 0, 0), (0, 0, 0), (1, 0, 0)\n",
    "    ]\n",
    "\n",
    "print(\"Configuration set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN MODEL\n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, n_frames, n_actions, h_dimension):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(\n",
    "            6, kernel_size=(7, 7), strides=(3, 3), activation='relu', data_format='channels_first')\n",
    "        self.pool1 = layers.MaxPooling2D(\n",
    "            pool_size=(2, 2), data_format='channels_first')\n",
    "        self.conv2 = layers.Conv2D(\n",
    "            12, kernel_size=(4, 4), activation='relu', data_format='channels_first')\n",
    "        self.pool2 = layers.MaxPooling2D(\n",
    "            pool_size=(2, 2), data_format='channels_first')\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc1 = layers.Dense(h_dimension, activation='relu')\n",
    "        self.fc2 = layers.Dense(n_actions)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        return self.fc2(x)\n",
    "print(\"DQN model defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN AGENT\n",
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                 action_space,\n",
    "                 epsilon=1.0,\n",
    "                 gamma=0.95,\n",
    "                 epsilon_min=0.1,\n",
    "                 epsilon_decay=0.9999,\n",
    "                 lr=1e-3,\n",
    "                 memory_len=5000,\n",
    "                 frames=3,\n",
    "                 hidden_dimension=150):\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=memory_len)\n",
    "        self.action_space = action_space\n",
    "        self.n_actions = len(action_space)\n",
    "\n",
    "        self.model = DQN(frames, self.n_actions, hidden_dimension)\n",
    "        self.target_model = DQN(frames, self.n_actions, hidden_dimension)\n",
    "        self.model.build((None, frames, 96, 96))\n",
    "        self.target_model.build((None, frames, 96, 96))\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.optimizer = optimizers.Adam(learning_rate=lr)\n",
    "        self.loss_fn = losses.MeanSquaredError()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def is_explore(self):\n",
    "        return random.random() < self.epsilon\n",
    "\n",
    "    def act(self, state, is_only_random=False, is_only_exploit=False):\n",
    "        if (not is_only_exploit and self.is_explore()) or is_only_random:\n",
    "            action_index = np.random.randint(self.n_actions)\n",
    "            print(f\"Exploring: Taking random action index {action_index}\")\n",
    "        else:\n",
    "            q_values_dict = self.target_model(state, training=False)\n",
    "            # Extract the tensor from the dict; assuming the key is 'output_1'\n",
    "            q_values = q_values_dict['output_1']\n",
    "            q_values = tf.squeeze(q_values)  # Shape: (n_actions,)\n",
    "            action_index = tf.argmax(q_values).numpy()\n",
    "            print(f\"Exploiting: Q-values {q_values.numpy()}, taking action index {action_index}\")\n",
    "        return self.action_space[action_index]\n",
    "\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        action_index = self.action_space.index(action)\n",
    "        self.memory.append((state, action_index, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model(states, training=True)\n",
    "            q_selected = tf.reduce_sum(\n",
    "                q_values * tf.one_hot(actions, self.n_actions), axis=1)\n",
    "\n",
    "            q_next = self.target_model(next_states, training=False)\n",
    "            max_q_next = tf.reduce_max(q_next, axis=1)\n",
    "            target = rewards + (1 - dones) * self.gamma * max_q_next\n",
    "\n",
    "            loss = self.loss_fn(target, q_selected)\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path, save_format=\"tf\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model = tf.keras.layers.TFSMLayer(path, call_endpoint='serving_default')\n",
    "        self.target_model = tf.keras.layers.TFSMLayer(path, call_endpoint='serving_default')\n",
    "        #self.model = tf.keras.models.load_model(path)\n",
    "        #self.target_model = tf.keras.models.load_model(path)\n",
    "\n",
    "print(\"DQN agent defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN TRAINING LOOP\n",
    "def train_car_racing():\n",
    "    seed_everything(Config.SEED)\n",
    "    path_root = make_all_paths(is_dynamic_root=True)\n",
    "    write_json_to_file(Config.__dict__, os.path.join(path_root, \"config.json\"))\n",
    "\n",
    "    agent = DQNAgent(\n",
    "        action_space=Config.ACTION_SPACE,\n",
    "        frames=Config.N_FRAMES,\n",
    "        hidden_dimension=Config.HIDDEN_DIMENSION_FC\n",
    "    )\n",
    "\n",
    "    env = gym.make('CarRacing-v3', render_mode=\"rgb_array\")\n",
    "    env = gym.wrappers.RecordVideo(\n",
    "        env, os.path.join(path_root, 'videos'),\n",
    "        episode_trigger=lambda x: x % Config.UPDATE_TARGET_MODEL_FREQUENCY == 0\n",
    "    )\n",
    "\n",
    "    epi_total_rewards = []\n",
    "    for episode in range(Config.STARTING_EPISODE_TRAIN, Config.ENDING_EPISODE_TRAIN + 1):\n",
    "        state, _ = env.reset(seed=episode)\n",
    "        state = preprocess_frame_car(state)\n",
    "        state_queue = deque([state] * Config.N_FRAMES, maxlen=Config.N_FRAMES)\n",
    "        epi_total_reward = 0\n",
    "        epi_negative_reward_counter = 0\n",
    "\n",
    "        while True:\n",
    "            state_array = np.array(state_queue)\n",
    "            state_tensor = tf.convert_to_tensor(state_array, dtype=tf.float32)[None, ...]\n",
    "\n",
    "            action = agent.act(state_tensor)\n",
    "            reward = 0\n",
    "            done = False\n",
    "\n",
    "            for _ in range(Config.SKIP_FRAMES):\n",
    "                next_state, r, done, truncated, _ = env.step(action)\n",
    "                reward += r\n",
    "                if done or truncated:\n",
    "                    break\n",
    "\n",
    "            if epi_total_reward > 100 and reward < 0:\n",
    "                epi_negative_reward_counter += 1\n",
    "            else:\n",
    "                epi_negative_reward_counter = 0\n",
    "\n",
    "            if action[1] == 1 and action[2] == 0:\n",
    "                reward *= Config.GAS_WEIGHT\n",
    "\n",
    "            epi_total_reward += reward\n",
    "\n",
    "            next_state = preprocess_frame_car(next_state)\n",
    "            state_queue.append(next_state)\n",
    "\n",
    "            next_state_array = np.array(state_queue)\n",
    "            next_state_tensor = tf.convert_to_tensor(next_state_array, dtype=tf.float32)[None, ...]\n",
    "\n",
    "            agent.memorize(state_tensor, action, reward, next_state_tensor, done)\n",
    "\n",
    "            if epi_negative_reward_counter >= 25 or epi_total_reward < 0 or done:\n",
    "                break\n",
    "\n",
    "            if len(agent.memory) > Config.TRAINING_BATCH_SIZE:\n",
    "                agent.replay(Config.TRAINING_BATCH_SIZE)\n",
    "\n",
    "        epi_total_rewards.append(epi_total_reward)\n",
    "        print(f\"Episode: {episode}/{Config.ENDING_EPISODE_TRAIN}, \"\n",
    "              f\"Total Reward: {epi_total_reward:.2f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "        if episode % Config.UPDATE_TARGET_MODEL_FREQUENCY == 0:\n",
    "            plt.plot(epi_total_rewards, label=\"Cumulative Reward\")\n",
    "            plt.title(\"Training Progress\")\n",
    "            plt.savefig(os.path.join(path_root, 'plots', f'reward_{episode}.pdf'))\n",
    "            plt.clf()\n",
    "\n",
    "            agent.save_model(os.path.join(path_root, \"models\", f\"model_{episode}\"))\n",
    "            agent.update_target_model()\n",
    "            write_json_to_file({\"CUM_REW\": epi_total_rewards}, os.path.join(path_root, \"stats.json\"))\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "print(\"Training loop defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST FUNCTION\n",
    "def test_car_racing(model_to_load):\n",
    "    seed_everything(Config.SEED)\n",
    "    agent = DQNAgent(\n",
    "        action_space=Config.ACTION_SPACE,\n",
    "        frames=Config.N_FRAMES,\n",
    "        hidden_dimension=Config.HIDDEN_DIMENSION_FC\n",
    "    )\n",
    "    agent.load_model(model_to_load)\n",
    "\n",
    "    env = gym.make('CarRacing-v3', render_mode=\"human\")\n",
    "    PICKED_EPISODES = [1]\n",
    "\n",
    "    for episode in PICKED_EPISODES:\n",
    "        state, _ = env.reset(seed=episode)\n",
    "        state = preprocess_frame_car(state)\n",
    "        state_queue = deque([state] * Config.N_FRAMES, maxlen=Config.N_FRAMES)\n",
    "        epi_negative_reward_counter = 0\n",
    "\n",
    "        while True:\n",
    "            state_array = np.array(state_queue)\n",
    "            state_tensor = tf.convert_to_tensor(state_array, dtype=tf.float32)[None, ...]\n",
    "\n",
    "            action = agent.act(state_tensor, is_only_exploit=True)\n",
    "            reward = 0\n",
    "            done = False\n",
    "\n",
    "            for _ in range(Config.SKIP_FRAMES):\n",
    "                next_state, r, done, truncated, _ = env.step(action)\n",
    "                reward += r\n",
    "                if done or truncated:\n",
    "                    break\n",
    "\n",
    "            if reward <= 0:\n",
    "                epi_negative_reward_counter += 1\n",
    "            else:\n",
    "                epi_negative_reward_counter = 0\n",
    "\n",
    "            if epi_negative_reward_counter >= 100:\n",
    "                break\n",
    "\n",
    "            next_state = preprocess_frame_car(next_state)\n",
    "            state_queue.append(next_state)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "print(\"Test function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Created dir reinforcement_learning/rl_class_20241210_132210/models\n",
      ">> Created dir reinforcement_learning/rl_class_20241210_132210/plots\n",
      ">> Created dir reinforcement_learning/rl_class_20241210_132210/videos\n",
      "JSON data successfully written to reinforcement_learning/rl_class_20241210_132210/config.json\n",
      ">> Using device: GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andri\\miniconda3\\envs\\tf\\lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\andri\\Documents\\GitHub\\Homework-2\\reinforcement_learning\\rl_class_20241210_132210\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0/50, Scores(Time Frames): 125, Total Rewards: 1.77, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_0\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_0\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data successfully written to reinforcement_learning/rl_class_20241210_132210/stats.json\n",
      "Episode: 1/50, Scores(Time Frames): 125, Total Rewards: 16.17, Epsilon: 0.98\n",
      "Episode: 2/50, Scores(Time Frames): 200, Total Rewards: 36.11, Epsilon: 0.96\n",
      "Episode: 3/50, Scores(Time Frames): 194, Total Rewards: 28.49, Epsilon: 0.94\n",
      "Episode: 4/50, Scores(Time Frames): 118, Total Rewards: -0.19, Epsilon: 0.93\n",
      "Episode: 5/50, Scores(Time Frames): 206, Total Rewards: 19.51, Epsilon: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_5\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data successfully written to reinforcement_learning/rl_class_20241210_132210/stats.json\n",
      "Episode: 6/50, Scores(Time Frames): 125, Total Rewards: 4.48, Epsilon: 0.90\n",
      "Episode: 7/50, Scores(Time Frames): 73, Total Rewards: -0.01, Epsilon: 0.89\n",
      "Episode: 8/50, Scores(Time Frames): 166, Total Rewards: 22.05, Epsilon: 0.88\n",
      "Episode: 9/50, Scores(Time Frames): 181, Total Rewards: 28.47, Epsilon: 0.86\n",
      "Episode: 10/50, Scores(Time Frames): 125, Total Rewards: 17.81, Epsilon: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_10\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_10\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data successfully written to reinforcement_learning/rl_class_20241210_132210/stats.json\n",
      "Episode: 11/50, Scores(Time Frames): 206, Total Rewards: 30.41, Epsilon: 0.84\n",
      "Episode: 12/50, Scores(Time Frames): 160, Total Rewards: 13.09, Epsilon: 0.82\n",
      "Episode: 13/50, Scores(Time Frames): 220, Total Rewards: 48.13, Epsilon: 0.80\n",
      "Episode: 14/50, Scores(Time Frames): 125, Total Rewards: 4.85, Epsilon: 0.79\n",
      "Episode: 15/50, Scores(Time Frames): 200, Total Rewards: 13.31, Epsilon: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_15\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_15\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data successfully written to reinforcement_learning/rl_class_20241210_132210/stats.json\n",
      "Episode: 16/50, Scores(Time Frames): 70, Total Rewards: -0.12, Epsilon: 0.77\n",
      "Episode: 17/50, Scores(Time Frames): 125, Total Rewards: 8.30, Epsilon: 0.76\n",
      "Episode: 18/50, Scores(Time Frames): 136, Total Rewards: -0.03, Epsilon: 0.75\n",
      "Episode: 19/50, Scores(Time Frames): 68, Total Rewards: -0.09, Epsilon: 0.75\n",
      "Episode: 20/50, Scores(Time Frames): 125, Total Rewards: 0.46, Epsilon: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_20\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_20\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data successfully written to reinforcement_learning/rl_class_20241210_132210/stats.json\n",
      "Episode: 21/50, Scores(Time Frames): 125, Total Rewards: 11.54, Epsilon: 0.73\n",
      "Episode: 22/50, Scores(Time Frames): 235, Total Rewards: 14.84, Epsilon: 0.71\n",
      "Episode: 23/50, Scores(Time Frames): 160, Total Rewards: 24.76, Epsilon: 0.70\n",
      "Episode: 24/50, Scores(Time Frames): 125, Total Rewards: 1.00, Epsilon: 0.69\n",
      "Episode: 25/50, Scores(Time Frames): 125, Total Rewards: 2.55, Epsilon: 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_25\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_25\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data successfully written to reinforcement_learning/rl_class_20241210_132210/stats.json\n",
      "Episode: 26/50, Scores(Time Frames): 133, Total Rewards: 53.19, Epsilon: 0.68\n",
      "Episode: 27/50, Scores(Time Frames): 125, Total Rewards: 16.69, Epsilon: 0.67\n",
      "Episode: 28/50, Scores(Time Frames): 125, Total Rewards: 19.43, Epsilon: 0.66\n",
      "Episode: 29/50, Scores(Time Frames): 146, Total Rewards: 32.44, Epsilon: 0.65\n",
      "Episode: 30/50, Scores(Time Frames): 118, Total Rewards: -0.16, Epsilon: 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_30\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_30\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data successfully written to reinforcement_learning/rl_class_20241210_132210/stats.json\n",
      "Episode: 31/50, Scores(Time Frames): 125, Total Rewards: 16.95, Epsilon: 0.63\n",
      "Episode: 32/50, Scores(Time Frames): 125, Total Rewards: 11.76, Epsilon: 0.63\n",
      "Episode: 33/50, Scores(Time Frames): 125, Total Rewards: 6.86, Epsilon: 0.62\n",
      "Episode: 34/50, Scores(Time Frames): 125, Total Rewards: 24.99, Epsilon: 0.61\n",
      "Episode: 35/50, Scores(Time Frames): 125, Total Rewards: 33.24, Epsilon: 0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_35\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_35\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data successfully written to reinforcement_learning/rl_class_20241210_132210/stats.json\n",
      "Episode: 36/50, Scores(Time Frames): 135, Total Rewards: 11.14, Epsilon: 0.60\n",
      "Episode: 37/50, Scores(Time Frames): 74, Total Rewards: -0.01, Epsilon: 0.59\n",
      "Episode: 38/50, Scores(Time Frames): 109, Total Rewards: -0.02, Epsilon: 0.58\n",
      "Episode: 39/50, Scores(Time Frames): 48, Total Rewards: -0.14, Epsilon: 0.58\n",
      "Episode: 40/50, Scores(Time Frames): 117, Total Rewards: -0.05, Epsilon: 0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_40\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_40\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data successfully written to reinforcement_learning/rl_class_20241210_132210/stats.json\n",
      "Episode: 41/50, Scores(Time Frames): 125, Total Rewards: 4.75, Epsilon: 0.57\n",
      "Episode: 42/50, Scores(Time Frames): 72, Total Rewards: -0.11, Epsilon: 0.56\n",
      "Episode: 43/50, Scores(Time Frames): 144, Total Rewards: 17.70, Epsilon: 0.56\n",
      "Episode: 44/50, Scores(Time Frames): 125, Total Rewards: 10.46, Epsilon: 0.55\n",
      "Episode: 45/50, Scores(Time Frames): 203, Total Rewards: 19.16, Epsilon: 0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_45\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_45\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data successfully written to reinforcement_learning/rl_class_20241210_132210/stats.json\n",
      "Episode: 46/50, Scores(Time Frames): 127, Total Rewards: 37.95, Epsilon: 0.53\n",
      "Episode: 47/50, Scores(Time Frames): 125, Total Rewards: 17.07, Epsilon: 0.53\n",
      "Episode: 48/50, Scores(Time Frames): 125, Total Rewards: 7.44, Epsilon: 0.52\n",
      "Episode: 49/50, Scores(Time Frames): 128, Total Rewards: 17.31, Epsilon: 0.51\n",
      "Episode: 50/50, Scores(Time Frames): 152, Total Rewards: 20.33, Epsilon: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_50\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: reinforcement_learning/rl_class_20241210_132210/models\\model_50\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data successfully written to reinforcement_learning/rl_class_20241210_132210/stats.json\n"
     ]
    }
   ],
   "source": [
    "# MAIN EXECUTION\n",
    "\n",
    "# To train:\n",
    "train_car_racing()\n",
    "# To test:\n",
    "# args = parse_args()\n",
    "# test_car_racing(model_to_load=args['mod'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
