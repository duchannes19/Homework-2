{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Iteration ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if GPU is available\n",
    "print('GPU is', 'available' if tf.config.list_physical_devices('GPU') else 'NOT AVAILABLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import numpy as np\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras import optimizers, callbacks\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from gymnasium.wrappers import RecordVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Preparation\n",
    "# Define the paths to your training and validation data\n",
    "trainingset = 'train/'\n",
    "validationset = 'test/'\n",
    "\n",
    "batch_size = 64\n",
    "target_size = (96, 96)  # Adjust based on your dataset\n",
    "\n",
    "# Training data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rotation_range=20,\n",
    "    shear_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation data should not be augmented\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=trainingset,\n",
    "    target_size=target_size,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    directory=validationset,\n",
    "    target_size=target_size,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Gather dataset information\n",
    "num_samples = train_generator.n\n",
    "num_classes = train_generator.num_classes\n",
    "input_shape = train_generator.image_shape\n",
    "\n",
    "classnames = list(train_generator.class_indices.keys())\n",
    "img_h, img_w, img_channels = input_shape\n",
    "print(f\"Image height = {img_h}, Image Width = {img_w}, Channels = {img_channels}\")\n",
    "print(f\"Image input shape: {input_shape}\")\n",
    "print(f\"Classes: {classnames}\")\n",
    "print(f\"Loaded {num_samples} training samples from {num_classes} classes.\")\n",
    "print(f\"Loaded {validation_generator.n} validation samples from {validation_generator.num_classes} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell X: Analyze Class Distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count of samples per class\n",
    "class_counts = train_generator.classes\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(class_names, [np.sum(class_counts == i) for i in range(len(class_names))], color='skyblue')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Create Separate Generators for Minority Classes\n",
    "import os\n",
    "\n",
    "# Define the target size and batch size\n",
    "target_size = (96, 96)\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize data augmentation for minority classes\n",
    "minority_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=30,\n",
    "    shear_range=0.2,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Paths to minority class directories\n",
    "minority_classes = ['0', '4']\n",
    "train_dir = 'train/'\n",
    "\n",
    "augmented_generators = []\n",
    "\n",
    "for cls in minority_classes:\n",
    "    class_dir = os.path.join(train_dir, cls)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        print(f\"Directory for class {cls} not found. Skipping augmentation.\")\n",
    "        continue\n",
    "    generator = minority_datagen.flow_from_directory(\n",
    "        directory=train_dir,\n",
    "        target_size=target_size,\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"categorical\",\n",
    "        classes=[cls],  # Only target the minority class\n",
    "        shuffle=True\n",
    "    )\n",
    "    augmented_generators.append(generator)\n",
    "\n",
    "print(f\"Number of augmented generators: {len(augmented_generators)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell Y: Compute Class Weights\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_generator.classes),\n",
    "    y=train_generator.classes\n",
    ")\n",
    "\n",
    "# Convert to a dictionary\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"Class Weights:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Combine Original and Augmented Generators\n",
    "import itertools\n",
    "\n",
    "# Create a combined generator\n",
    "train_combined = itertools.chain(\n",
    "    train_generator,\n",
    "    *augmented_generators\n",
    ")\n",
    "\n",
    "# Define a generator that yields data from the combined generator\n",
    "def combined_generator(combined):\n",
    "    for data in combined:\n",
    "        yield data\n",
    "\n",
    "# Reset the iterator\n",
    "train_combined = combined_generator(train_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Deeper CNN with Different Activation\n",
    "def DeepCNN(input_shape, num_classes):\n",
    "    model = Sequential(name=\"DeepCNN\")\n",
    "\n",
    "    # First Convolutional Block\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # Flatten and Dense Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile the model with RMSprop optimizer\n",
    "    optimizer = optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the model\n",
    "model_deep = DeepCNN(input_shape, num_classes)\n",
    "model_deep.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "def ImprovedDeepCNN(input_shape, num_classes):\n",
    "    model = models.Sequential(name=\"ImprovedDeepCNN\")\n",
    "    \n",
    "    # First Convolutional Block\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(layers.Conv2D(32, (3, 3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # Third Convolutional Block\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    \n",
    "    # Global Average Pooling and Dense Layers\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(512, kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile the model with Adam optimizer\n",
    "    optimizer = optimizers.Adam(learning_rate=0.0001)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Instantiate and summarize the improved model\n",
    "model_improved = ImprovedDeepCNN(input_shape, num_classes)\n",
    "model_improved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Select Model\n",
    "# Choose one of the models defined above\n",
    "model = model_improved  # Replace with model_deep or model_improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Define Callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Increased patience for potentially longer training\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,  # Increased patience\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Optionally, add ModelCheckpoint to save the best model\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    'models/best_model.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Train the Model\n",
    "# Calculate steps per epoch and validation steps\n",
    "steps_per_epoch = int(np.ceil(train_generator.n / batch_size))\n",
    "val_steps = int(np.ceil(validation_generator.n / batch_size))\n",
    "\n",
    "try:\n",
    "    history = model.fit(\n",
    "        train_combined,\n",
    "        epochs=100,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "        class_weight=class_weights_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted.\")\n",
    "\n",
    "# Save the final model based if you chose deep or improved model\n",
    "if model == model_deep:\n",
    "    model.save('models/final_model_deep.keras')\n",
    "else:\n",
    "    model.save('models/final_model_improved.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Average\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_deep = load_model('models/final_model_deep.keras')\n",
    "model_improved = load_model('models/final_model_improved.keras')\n",
    "\n",
    "input_shape_deep = model_deep.input_shape[1:]  # Exclude batch size\n",
    "input_shape_improved = model_improved.input_shape[1:]\n",
    "\n",
    "assert input_shape_deep == input_shape_improved, \"Input shapes of both models must be the same.\"\n",
    "\n",
    "# Create an input layer that matches the input shape\n",
    "input_layer = Input(shape=input_shape_deep)\n",
    "\n",
    "# Get predictions from both models\n",
    "preds_deep = model_deep(input_layer)\n",
    "preds_improved = model_improved(input_layer)\n",
    "\n",
    "# Average the outputs\n",
    "averaged_preds = Average()([preds_deep, preds_improved])\n",
    "\n",
    "# Create the combined model\n",
    "combined_model = Model(inputs=input_layer, outputs=averaged_preds)\n",
    "\n",
    "# Compile the combined model\n",
    "combined_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Save the combined model\n",
    "combined_model.save('models/combined_model.keras')\n",
    "\n",
    "# Cell 11: Evaluate the Model\n",
    "# Load the combined model\n",
    "combined_model = load_model('models/combined_model.keras')\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_loss, val_acc = combined_model.evaluate(validation_generator, verbose=1)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Evaluate the Model\n",
    "# Load the best saved model\n",
    "best_model = load_model('models/combined_model.keras')\n",
    "\n",
    "# Evaluate on validation data\n",
    "val_steps = int(np.ceil(validation_generator.n / batch_size))\n",
    "loss, acc = best_model.evaluate(validation_generator, steps=val_steps, verbose=1)\n",
    "print(f'Loss: {loss:.4f}')\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Classification Report\n",
    "preds = best_model.predict(validation_generator, steps=val_steps, verbose=0)\n",
    "Ypred = np.argmax(preds, axis=1)\n",
    "Ytest = validation_generator.classes  # Ensure shuffle=False in validation_generator\n",
    "\n",
    "print(classification_report(Ytest, Ypred, target_names=classnames, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Confusion Matrix\n",
    "cm = confusion_matrix(Ytest, Ypred)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=classnames, yticklabels=classnames, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Text-Based Confusion Matrix\n",
    "conf = []\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        if i != j and cm[i][j] > 0:\n",
    "            conf.append([i, j, cm[i][j]])\n",
    "\n",
    "conf = np.array(conf)\n",
    "conf = conf[np.argsort(-conf[:, 2])]  # Sort by descending error count\n",
    "\n",
    "print(f'{\"True\":<16} {\"Predicted\":<16} {\"Errors\":<10} {\"Error %\":<10}')\n",
    "print('-' * 60)\n",
    "for k in conf:\n",
    "    true_class = classnames[int(k[0])]\n",
    "    pred_class = classnames[int(k[1])]\n",
    "    errors = int(k[2])\n",
    "    error_pct = (errors / validation_generator.n) * 100\n",
    "    print(f'{true_class:<16} -> {pred_class:<16} {errors:<10} {error_pct:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Model Deployment with Gymnasium (Final Revised for Continuous Actions)\n",
    "import numpy as np\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "def play(env, model, predefined_actions):\n",
    "    seed = 2000\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "\n",
    "    # Drop initial frames with no action\n",
    "    no_action = predefined_actions[0]  # [0.0, 0.0, 0.0]\n",
    "    for _ in range(50):\n",
    "        obs, _, _, _, _ = env.step(no_action)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Preprocess the observation\n",
    "        img = preprocess_observation(obs, target_size)\n",
    "        p = model.predict(np.expand_dims(img, axis=0))  # Shape: (1, 5)\n",
    "        predicted_class = np.argmax(p)  # Integer 0-4\n",
    "\n",
    "        # Map the predicted class to a predefined action\n",
    "        action = predefined_actions.get(predicted_class, predefined_actions[0])  # Array\n",
    "\n",
    "        # Ensure the action is a float32 NumPy array\n",
    "        action = action.astype(np.float32)\n",
    "\n",
    "        # Step the environment with the action\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    env.close()\n",
    "\n",
    "def preprocess_observation(obs, target_size):\n",
    "    from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
    "\n",
    "    # Convert observation to PIL Image\n",
    "    img = array_to_img(obs)\n",
    "    # Resize image\n",
    "    img = img.resize(target_size)\n",
    "    # Convert to array and normalize\n",
    "    img = img_to_array(img) / 255.0\n",
    "    return img\n",
    "\n",
    "# Define predefined actions (Continuous)\n",
    "predefined_actions = {\n",
    "    0: np.array([0.0, 0.0, 0.0], dtype=np.float32),  # No Action\n",
    "    1: np.array([-1.0, 0.0, 0.0], dtype=np.float32), # Steer Left\n",
    "    2: np.array([1.0, 0.0, 0.0], dtype=np.float32),  # Steer Right\n",
    "    3: np.array([0.0, 1.0, 0.0], dtype=np.float32),  # Accelerate (Gas)\n",
    "    4: np.array([0.0, 0.0, 1.0], dtype=np.float32),  # Brake\n",
    "    # Add more actions as needed\n",
    "}\n",
    "\n",
    "# Initialize the environment without 'continuous' parameter\n",
    "env_arguments = {\n",
    "    'domain_randomize': False,\n",
    "    'render_mode': 'rgb_array'\n",
    "}\n",
    "\n",
    "env_name = 'CarRacing-v3'\n",
    "env = gym.make(env_name, **env_arguments)\n",
    "\n",
    "# Wrap the environment to record videos\n",
    "video_dir = 'video_recordings'  # Specify the directory to save video recordings\n",
    "env = RecordVideo(env, video_dir)\n",
    "\n",
    "print(\"Environment:\", env_name)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "\n",
    "# Play the game using the trained model\n",
    "play(env, best_model, predefined_actions)\n",
    "#play(env, best_model_regression, predefined_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional Approach) Reinforcement Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from collections import deque\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Suppress TensorFlow warnings for cleaner output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1. Define Predefined Actions\n",
    "# =========================\n",
    "predefined_actions = {\n",
    "    0: np.array([0.0, 0.0, 0.0], dtype=np.float32),  # No Action\n",
    "    1: np.array([-1.0, 0.0, 0.0], dtype=np.float32), # Steer Left\n",
    "    2: np.array([1.0, 0.0, 0.0], dtype=np.float32),  # Steer Right\n",
    "    3: np.array([0.0, 1.0, 0.0], dtype=np.float32),  # Accelerate (Gas)\n",
    "    4: np.array([0.0, 0.0, 1.0], dtype=np.float32),  # Brake\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2. Define the Gym Environment\n",
    "# =========================\n",
    "class SimpleCarEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Gymnasium Environment for Car Driving with Basic Dynamics.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SimpleCarEnv, self).__init__()\n",
    "        # Define action space\n",
    "        self.action_space = spaces.Discrete(len(predefined_actions))\n",
    "        # Define observation space for preprocessed frames (RGB)\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(96, 96, 3), dtype=np.float32)\n",
    "        self.max_steps = 100\n",
    "        self.current_step = 0\n",
    "        self.position = np.array([48, 48], dtype=np.float32)  # Start at center (96/2, 96/2)\n",
    "        self.speed = 0.0\n",
    "        self.direction = 0.0  # Angle in degrees\n",
    "        self.track_radius = 40  # Simple circular track\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to an initial state and return the initial observation.\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        self.position = np.array([48, 48], dtype=np.float32)\n",
    "        self.speed = 0.0\n",
    "        self.direction = 0.0\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Apply the action to the environment and return the next observation, reward, done, and info.\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "    \n",
    "        # Apply action effects\n",
    "        if action == 1:  # Steer Left\n",
    "            self.direction -= 15  # degrees\n",
    "        elif action == 2:  # Steer Right\n",
    "            self.direction += 15  # degrees\n",
    "        elif action == 3:  # Accelerate\n",
    "            self.speed += 0.5\n",
    "        elif action == 4:  # Brake\n",
    "            self.speed -= 0.5\n",
    "            self.speed = max(self.speed, 0.0)  # Prevent negative speed\n",
    "    \n",
    "        # Ensure direction is within [0, 360)\n",
    "        self.direction %= 360\n",
    "    \n",
    "        # Update position based on speed and direction\n",
    "        rad = np.deg2rad(self.direction)\n",
    "        delta_x = self.speed * np.cos(rad)\n",
    "        delta_y = self.speed * np.sin(rad)\n",
    "        self.position += np.array([delta_x, delta_y], dtype=np.float32)\n",
    "    \n",
    "        # Calculate distance from center (assuming center is (48, 48))\n",
    "        distance = np.linalg.norm(self.position - np.array([48, 48], dtype=np.float32))\n",
    "    \n",
    "        # Determine if off track\n",
    "        done = False\n",
    "        reward = 1.0  # Reward for staying on track\n",
    "    \n",
    "        if distance > self.track_radius:\n",
    "            done = True\n",
    "            reward = -100.0  # Off-track penalty\n",
    "    \n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "            reward = 100.0  # Success reward\n",
    "    \n",
    "        # Generate observation\n",
    "        observation = self._get_observation()\n",
    "    \n",
    "        info = {\n",
    "            'collision': False,  # Placeholder for collision detection\n",
    "            'off_track': distance > self.track_radius,\n",
    "            'speed': self.speed\n",
    "        }\n",
    "    \n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Render the environment to the screen.\n",
    "        \"\"\"\n",
    "        # Optional: Implement visualization using OpenCV or other libraries\n",
    "        pass\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Generate and return the current observation.\n",
    "        \"\"\"\n",
    "        # Create a blank RGB image\n",
    "        img = np.zeros((240, 320, 3), dtype=np.float32)  # Using float32 for consistency\n",
    "        \n",
    "        # Draw the track (simple circular track)\n",
    "        center_coordinates = (160, 120)  # Center of the image\n",
    "        cv2.circle(img, center_coordinates, self.track_radius * 4, (0, 255, 0), 2)  # Green track\n",
    "        \n",
    "        # Draw the car's current position\n",
    "        # Scale position from (0, 96) to image coordinates (0, 320) for x and (0, 96) to (0, 240) for y\n",
    "        pos_x = int(self.position[0] * (320 / 96))\n",
    "        pos_y = int(self.position[1] * (240 / 96))\n",
    "        pos_x = np.clip(pos_x, 0, 319)\n",
    "        pos_y = np.clip(pos_y, 0, 239)\n",
    "        cv2.circle(img, (pos_x, pos_y), 5, (255, 0, 0), -1)  # Red car\n",
    "        \n",
    "        # Normalize pixel values to [0.0, 1.0]\n",
    "        normalized = img / 255.0\n",
    "        \n",
    "        # Resize to (96, 96)\n",
    "        resized = cv2.resize(normalized, (96, 96), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        return resized  # Shape: (96, 96, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3. Preprocessing Functions\n",
    "# =========================\n",
    "def preprocess_observation(observation):\n",
    "    \"\"\"\n",
    "    Preprocess the observation by performing any additional preprocessing steps if needed.\n",
    "    Currently, the observation is already preprocessed within the environment.\n",
    "    \"\"\"\n",
    "    # Placeholder: Additional preprocessing can be added here\n",
    "    return observation\n",
    "\n",
    "def compute_reward(info, done):\n",
    "    \"\"\"\n",
    "    Compute the reward based on the current state.\n",
    "    \n",
    "    Parameters:\n",
    "    - info: Dictionary containing additional information about the environment.\n",
    "    - done: Boolean indicating if the episode has ended.\n",
    "    \n",
    "    Returns:\n",
    "    - reward: Float value representing the reward.\n",
    "    \"\"\"\n",
    "    if done:\n",
    "        if info.get('collision', False):\n",
    "            return -100.0  # Collision penalty\n",
    "        elif info.get('off_track', False):\n",
    "            return -100.0  # Off track penalty\n",
    "        else:\n",
    "            return 100.0  # Success reward\n",
    "    else:\n",
    "        reward = 1.0  # Reward for staying on track\n",
    "        # Optionally, add more components (e.g., speed)\n",
    "        reward += info.get('speed', 0.0) * 0.1\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4. Frame Stacking Wrapper\n",
    "# =========================\n",
    "class FrameStackEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Gymnasium environment wrapper to stack consecutive frames.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, stack_size=4):\n",
    "        super(FrameStackEnv, self).__init__(env)\n",
    "        self.stack_size = stack_size\n",
    "        self.frames = deque(maxlen=stack_size)\n",
    "        # Update the observation space to reflect stacked frames\n",
    "        low = np.repeat(env.observation_space.low, stack_size, axis=2)\n",
    "        high = np.repeat(env.observation_space.high, stack_size, axis=2)\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0,\n",
    "                                            shape=(env.observation_space.shape[0],\n",
    "                                                   env.observation_space.shape[1],\n",
    "                                                   env.observation_space.shape[2] * stack_size),\n",
    "                                            dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        for _ in range(self.stack_size):\n",
    "            self.frames.append(observation)\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(observation)\n",
    "        return self._get_observation(), reward, done, info\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return np.concatenate(list(self.frames), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5. Replay Buffer\n",
    "# =========================\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6. DQN Model Architecture\n",
    "# =========================\n",
    "def build_dqn_model(input_shape, num_actions):\n",
    "    \"\"\"\n",
    "    Build a Convolutional Neural Network for the DQN agent.\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name=\"DQN_Model\")\n",
    "\n",
    "    # Convolutional Layers\n",
    "    model.add(layers.Conv2D(32, (8, 8), strides=4, activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.Conv2D(64, (4, 4), strides=2, activation='relu'))\n",
    "    model.add(layers.Conv2D(64, (3, 3), strides=1, activation='relu'))\n",
    "\n",
    "    # Flatten and Dense Layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(num_actions, activation='linear'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7. DQN Agent\n",
    "# =========================\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_shape, num_actions, learning_rate=1e-4, gamma=0.99,\n",
    "                 epsilon=1.0, epsilon_min=0.1, epsilon_decay=1e-6, batch_size=32,\n",
    "                 replay_buffer_size=100000, target_update_freq=1000):\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # Main and Target Networks\n",
    "        self.model = build_dqn_model(input_shape, num_actions)\n",
    "        self.target_model = build_dqn_model(input_shape, num_actions)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        # Optimizer and Loss Function\n",
    "        self.optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.loss_function = tf.keras.losses.Huber()\n",
    "\n",
    "        # Replay Buffer\n",
    "        self.replay_buffer = ReplayBuffer(max_size=replay_buffer_size)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy.\n",
    "        \n",
    "        Parameters:\n",
    "        - state: Numpy array representing the current state.\n",
    "        \n",
    "        Returns:\n",
    "        - action: Integer representing the chosen action.\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        state = np.expand_dims(state, axis=0)  # Add batch dimension\n",
    "        q_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store experience tuple in the replay buffer.\n",
    "        \n",
    "        Parameters:\n",
    "        - state: Current state.\n",
    "        - action: Action taken.\n",
    "        - reward: Reward received.\n",
    "        - next_state: Next state after action.\n",
    "        - done: Boolean indicating if the episode has ended.\n",
    "        \"\"\"\n",
    "        self.replay_buffer.add((state, action, reward, next_state, done))\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay epsilon after each training step to reduce exploration over time.\n",
    "        \"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"\n",
    "        Sample a batch from the replay buffer and perform a training step.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "\n",
    "        # Predict Q-values for next states using target network\n",
    "        target_q = self.target_model.predict(next_states, verbose=0)\n",
    "        max_target_q = np.max(target_q, axis=1)\n",
    "\n",
    "        # Compute target values\n",
    "        targets = rewards + (1 - dones) * self.gamma * max_target_q\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model(states, training=True)\n",
    "            # Gather the Q-values for the taken actions\n",
    "            action_masks = tf.one_hot(actions, self.num_actions)\n",
    "            q_action = tf.reduce_sum(q_values * action_masks, axis=1)\n",
    "            loss = self.loss_function(targets, q_action)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        # Update epsilon\n",
    "        self.update_epsilon()\n",
    "\n",
    "        # Update target network\n",
    "        self.learn_step_counter += 1\n",
    "        if self.learn_step_counter % self.target_update_freq == 0:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Save the model's weights to the specified path.\n",
    "        \n",
    "        Parameters:\n",
    "        - path: String representing the file path to save the weights.\n",
    "        \"\"\"\n",
    "        self.model.save_weights(path)\n",
    "        print(f\"Model weights saved to {path}\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"\n",
    "        Load the model's weights from the specified path.\n",
    "        \n",
    "        Parameters:\n",
    "        - path: String representing the file path to load the weights from.\n",
    "        \"\"\"\n",
    "        self.model.load_weights(path)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        print(f\"Model weights loaded from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 8. Training and Evaluation Functions\n",
    "# =========================\n",
    "def train_dqn(env, agent, num_episodes=1000, max_steps=1000, save_every=100, save_path='dqn_final_model.h5'):\n",
    "    \"\"\"\n",
    "    Train the DQN agent in the given environment.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The Gym environment.\n",
    "    - agent: The DQN agent.\n",
    "    - num_episodes: Total number of training episodes.\n",
    "    - max_steps: Maximum steps per episode.\n",
    "    - save_every: Frequency (in episodes) to save the model.\n",
    "    - save_path: Path to save the final model.\n",
    "    \"\"\"\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            agent.train_step()\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(f\"Episode {episode}/{num_episodes} - Total Reward: {total_reward:.2f} - Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "        # Save the model at regular intervals\n",
    "        if episode % save_every == 0:\n",
    "            agent.save_model(f'models/reinforcement/dqn_model_episode_{episode}.weights.h5')\n",
    "\n",
    "    # Save the final model\n",
    "    agent.save_model(save_path)\n",
    "\n",
    "def evaluate_agent(env, agent, num_episodes=10, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Evaluate the trained DQN agent without exploration.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The Gym environment.\n",
    "    - agent: The DQN agent.\n",
    "    - num_episodes: Number of evaluation episodes.\n",
    "    - max_steps: Maximum steps per episode.\n",
    "\n",
    "    Returns:\n",
    "    - avg_reward: Average reward over the evaluation episodes.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0  # Disable exploration\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Evaluation Episode {episode}/{num_episodes} - Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "    agent.epsilon = original_epsilon  # Restore original epsilon\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Evaluation Reward over {num_episodes} episodes: {avg_reward:.2f}\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (96, 96, 12)\n",
      "Episode 1/100 - Total Reward: -40.00 - Epsilon: 1.0000\n",
      "Episode 2/100 - Total Reward: 199.00 - Epsilon: 0.9999\n",
      "Episode 3/100 - Total Reward: -67.00 - Epsilon: 0.9998\n",
      "Episode 4/100 - Total Reward: -35.00 - Epsilon: 0.9998\n",
      "Episode 5/100 - Total Reward: 199.00 - Epsilon: 0.9997\n",
      "Episode 6/100 - Total Reward: 199.00 - Epsilon: 0.9996\n",
      "Episode 7/100 - Total Reward: -18.00 - Epsilon: 0.9995\n",
      "Episode 8/100 - Total Reward: -59.00 - Epsilon: 0.9994\n",
      "Episode 9/100 - Total Reward: -34.00 - Epsilon: 0.9994\n",
      "Episode 10/100 - Total Reward: -54.00 - Epsilon: 0.9993\n",
      "Model weights saved to dqn_model_episode_10.weights.h5\n",
      "Episode 11/100 - Total Reward: -47.00 - Epsilon: 0.9993\n",
      "Episode 12/100 - Total Reward: -58.00 - Epsilon: 0.9992\n",
      "Episode 13/100 - Total Reward: 199.00 - Epsilon: 0.9991\n",
      "Episode 14/100 - Total Reward: -54.00 - Epsilon: 0.9991\n",
      "Episode 15/100 - Total Reward: -79.00 - Epsilon: 0.9991\n",
      "Episode 16/100 - Total Reward: -68.00 - Epsilon: 0.9990\n",
      "Episode 17/100 - Total Reward: -32.00 - Epsilon: 0.9990\n",
      "Episode 18/100 - Total Reward: 199.00 - Epsilon: 0.9989\n",
      "Episode 19/100 - Total Reward: -44.00 - Epsilon: 0.9988\n",
      "Episode 20/100 - Total Reward: -66.00 - Epsilon: 0.9988\n",
      "Model weights saved to dqn_model_episode_20.weights.h5\n",
      "Episode 21/100 - Total Reward: -71.00 - Epsilon: 0.9987\n",
      "Episode 22/100 - Total Reward: -70.00 - Epsilon: 0.9987\n",
      "Episode 23/100 - Total Reward: -22.00 - Epsilon: 0.9986\n",
      "Episode 24/100 - Total Reward: -59.00 - Epsilon: 0.9986\n",
      "Episode 25/100 - Total Reward: -22.00 - Epsilon: 0.9985\n",
      "Episode 26/100 - Total Reward: -49.00 - Epsilon: 0.9985\n",
      "Episode 27/100 - Total Reward: -59.00 - Epsilon: 0.9984\n",
      "Episode 28/100 - Total Reward: -25.00 - Epsilon: 0.9983\n",
      "Episode 29/100 - Total Reward: -57.00 - Epsilon: 0.9983\n",
      "Episode 30/100 - Total Reward: -65.00 - Epsilon: 0.9983\n",
      "Model weights saved to dqn_model_episode_30.weights.h5\n",
      "Episode 31/100 - Total Reward: -63.00 - Epsilon: 0.9982\n",
      "Episode 32/100 - Total Reward: -13.00 - Epsilon: 0.9981\n",
      "Episode 33/100 - Total Reward: -41.00 - Epsilon: 0.9981\n",
      "Episode 34/100 - Total Reward: -7.00 - Epsilon: 0.9980\n",
      "Episode 35/100 - Total Reward: -69.00 - Epsilon: 0.9979\n",
      "Episode 36/100 - Total Reward: -70.00 - Epsilon: 0.9979\n",
      "Episode 37/100 - Total Reward: -37.00 - Epsilon: 0.9979\n",
      "Episode 38/100 - Total Reward: -7.00 - Epsilon: 0.9978\n",
      "Episode 39/100 - Total Reward: -4.00 - Epsilon: 0.9977\n",
      "Episode 40/100 - Total Reward: -19.00 - Epsilon: 0.9976\n",
      "Model weights saved to dqn_model_episode_40.weights.h5\n",
      "Episode 41/100 - Total Reward: -49.00 - Epsilon: 0.9975\n",
      "Episode 42/100 - Total Reward: -45.00 - Epsilon: 0.9975\n",
      "Episode 43/100 - Total Reward: -58.00 - Epsilon: 0.9974\n",
      "Episode 44/100 - Total Reward: -71.00 - Epsilon: 0.9974\n",
      "Episode 45/100 - Total Reward: -46.00 - Epsilon: 0.9973\n",
      "Episode 46/100 - Total Reward: -33.00 - Epsilon: 0.9973\n",
      "Episode 47/100 - Total Reward: -37.00 - Epsilon: 0.9972\n",
      "Episode 48/100 - Total Reward: -62.00 - Epsilon: 0.9972\n",
      "Episode 49/100 - Total Reward: -72.00 - Epsilon: 0.9971\n",
      "Episode 50/100 - Total Reward: -58.00 - Epsilon: 0.9971\n",
      "Model weights saved to dqn_model_episode_50.weights.h5\n",
      "Episode 51/100 - Total Reward: -60.00 - Epsilon: 0.9971\n",
      "Episode 52/100 - Total Reward: -15.00 - Epsilon: 0.9970\n",
      "Episode 53/100 - Total Reward: -66.00 - Epsilon: 0.9969\n",
      "Episode 54/100 - Total Reward: -58.00 - Epsilon: 0.9969\n",
      "Episode 55/100 - Total Reward: -81.00 - Epsilon: 0.9969\n",
      "Episode 56/100 - Total Reward: -68.00 - Epsilon: 0.9968\n",
      "Episode 57/100 - Total Reward: -45.00 - Epsilon: 0.9968\n",
      "Episode 58/100 - Total Reward: -49.00 - Epsilon: 0.9967\n",
      "Episode 59/100 - Total Reward: 199.00 - Epsilon: 0.9966\n",
      "Episode 60/100 - Total Reward: -59.00 - Epsilon: 0.9966\n",
      "Model weights saved to dqn_model_episode_60.weights.h5\n",
      "Episode 61/100 - Total Reward: -45.00 - Epsilon: 0.9965\n",
      "Episode 62/100 - Total Reward: -26.00 - Epsilon: 0.9965\n",
      "Episode 63/100 - Total Reward: -15.00 - Epsilon: 0.9964\n",
      "Episode 64/100 - Total Reward: -71.00 - Epsilon: 0.9963\n",
      "Episode 65/100 - Total Reward: -47.00 - Epsilon: 0.9963\n",
      "Episode 66/100 - Total Reward: -13.00 - Epsilon: 0.9962\n",
      "Episode 67/100 - Total Reward: -56.00 - Epsilon: 0.9962\n",
      "Episode 68/100 - Total Reward: -26.00 - Epsilon: 0.9961\n",
      "Episode 69/100 - Total Reward: -44.00 - Epsilon: 0.9960\n",
      "Episode 70/100 - Total Reward: -37.00 - Epsilon: 0.9960\n",
      "Model weights saved to dqn_model_episode_70.weights.h5\n",
      "Episode 71/100 - Total Reward: -34.00 - Epsilon: 0.9959\n",
      "Episode 72/100 - Total Reward: -61.00 - Epsilon: 0.9959\n",
      "Episode 73/100 - Total Reward: -58.00 - Epsilon: 0.9958\n",
      "Episode 74/100 - Total Reward: -27.00 - Epsilon: 0.9957\n",
      "Episode 75/100 - Total Reward: 199.00 - Epsilon: 0.9956\n",
      "Episode 76/100 - Total Reward: -47.00 - Epsilon: 0.9956\n",
      "Episode 77/100 - Total Reward: -21.00 - Epsilon: 0.9955\n",
      "Episode 78/100 - Total Reward: -48.00 - Epsilon: 0.9955\n",
      "Episode 79/100 - Total Reward: -53.00 - Epsilon: 0.9954\n",
      "Episode 80/100 - Total Reward: -4.00 - Epsilon: 0.9953\n",
      "Model weights saved to dqn_model_episode_80.weights.h5\n",
      "Episode 81/100 - Total Reward: -66.00 - Epsilon: 0.9953\n",
      "Episode 82/100 - Total Reward: -43.00 - Epsilon: 0.9952\n",
      "Episode 83/100 - Total Reward: -54.00 - Epsilon: 0.9952\n",
      "Episode 84/100 - Total Reward: -18.00 - Epsilon: 0.9951\n",
      "Episode 85/100 - Total Reward: -42.00 - Epsilon: 0.9950\n",
      "Episode 86/100 - Total Reward: -34.00 - Epsilon: 0.9950\n",
      "Episode 87/100 - Total Reward: 199.00 - Epsilon: 0.9949\n",
      "Episode 88/100 - Total Reward: -55.00 - Epsilon: 0.9948\n",
      "Episode 89/100 - Total Reward: -37.00 - Epsilon: 0.9947\n",
      "Episode 90/100 - Total Reward: -51.00 - Epsilon: 0.9947\n",
      "Model weights saved to dqn_model_episode_90.weights.h5\n",
      "Episode 91/100 - Total Reward: -39.00 - Epsilon: 0.9946\n",
      "Episode 92/100 - Total Reward: -61.00 - Epsilon: 0.9946\n",
      "Episode 93/100 - Total Reward: -45.00 - Epsilon: 0.9945\n",
      "Episode 94/100 - Total Reward: -34.00 - Epsilon: 0.9945\n",
      "Episode 95/100 - Total Reward: -16.00 - Epsilon: 0.9944\n",
      "Episode 96/100 - Total Reward: 199.00 - Epsilon: 0.9943\n",
      "Episode 97/100 - Total Reward: -53.00 - Epsilon: 0.9942\n",
      "Episode 98/100 - Total Reward: -64.00 - Epsilon: 0.9942\n",
      "Episode 99/100 - Total Reward: -65.00 - Epsilon: 0.9942\n",
      "Episode 100/100 - Total Reward: -70.00 - Epsilon: 0.9941\n",
      "Model weights saved to dqn_model_episode_100.weights.h5\n",
      "Model weights saved to models/reinforcement/dqn_final_model.weights.h5\n",
      "Evaluation Episode 1/10 - Total Reward: 199.00\n",
      "Evaluation Episode 2/10 - Total Reward: 199.00\n",
      "Evaluation Episode 3/10 - Total Reward: 199.00\n",
      "Evaluation Episode 4/10 - Total Reward: 199.00\n",
      "Evaluation Episode 5/10 - Total Reward: 199.00\n",
      "Evaluation Episode 6/10 - Total Reward: 199.00\n",
      "Evaluation Episode 7/10 - Total Reward: 199.00\n",
      "Evaluation Episode 8/10 - Total Reward: 199.00\n",
      "Evaluation Episode 9/10 - Total Reward: 199.00\n",
      "Evaluation Episode 10/10 - Total Reward: 199.00\n",
      "Average Evaluation Reward over 10 episodes: 199.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(199.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# 9. Main Execution\n",
    "# =========================\n",
    "\n",
    "\n",
    "# Check for directory and create if it doesn't exist\n",
    "if not os.path.exists('models/reinforcement'):\n",
    "    os.makedirs('models/reinforcement')\n",
    "\n",
    "# Initialize Environment and Wrap with FrameStack\n",
    "env = SimpleCarEnv()\n",
    "env = FrameStackEnv(env, stack_size=4)\n",
    "# Define Input Shape and Number of Actions\n",
    "input_shape = env.observation_space.shape  # Should be (96, 96, 3)\n",
    "num_actions = env.action_space.n  # 5\n",
    "print(f\"Input Shape: {input_shape}\")  # For debugging\n",
    "# Initialize DQN Agent\n",
    "agent = DQNAgent(\n",
    "    input_shape=input_shape,\n",
    "    num_actions=num_actions,\n",
    "    learning_rate=1e-4,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_min=0.1,\n",
    "    epsilon_decay=1e-6,\n",
    "    batch_size=32,\n",
    "    replay_buffer_size=100000,\n",
    "    target_update_freq=1000\n",
    ")\n",
    "# Define Save Path (ensure this directory exists or change to a valid path)\n",
    "save_path = 'models/reinforcement/dqn_final_model.weights.h5'\n",
    "# Train the Agent\n",
    "train_dqn(env, agent, num_episodes=100, max_steps=100, save_every=10, save_path=save_path)\n",
    "# Evaluate the Agent\n",
    "evaluate_agent(env, agent, num_episodes=10, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded from models/reinforcement/dqn_final_model.weights.h5\n",
      "Environment: CarRacing-v3\n",
      "Action space: Box([-1.  0.  0.], 1.0, (3,), float32)\n",
      "Observation space: Box(0, 255, (96, 96, 3), uint8)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
    "\n",
    "target_size = (96, 96)\n",
    "\n",
    "def play(env, agent, predefined_actions):\n",
    "    seed = 2000\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "\n",
    "    # Drop initial frames with no action\n",
    "    no_action = predefined_actions[0]  # [0.0, 0.0, 0.0]\n",
    "    for _ in range(50):\n",
    "        obs, _, _, _, _ = env.step(no_action)\n",
    "\n",
    "    done = False\n",
    "    frames = deque(maxlen=4)  # Use deque to store the last 4 frames\n",
    "    while not done:\n",
    "        # Preprocess the observation\n",
    "        img = preprocess_observation(obs, target_size)\n",
    "        frames.append(img)\n",
    "        if len(frames) < 4:\n",
    "            continue  # Skip until we have 4 frames\n",
    "\n",
    "        stacked_frames = np.concatenate(list(frames), axis=-1)\n",
    "        stacked_frames = np.expand_dims(stacked_frames, axis=0)  # Add batch dimension\n",
    "        p = agent.model.predict(stacked_frames, verbose=0)\n",
    "        predicted_class = np.argmax(p)  # Integer 0-4\n",
    "\n",
    "        # Map the predicted class to a predefined action\n",
    "        action = predefined_actions.get(predicted_class, predefined_actions[0])  # Array\n",
    "\n",
    "        # Ensure the action is a float32 NumPy array\n",
    "        action = action.astype(np.float32)\n",
    "\n",
    "        # Step the environment with the action\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    env.close()\n",
    "\n",
    "def preprocess_observation(obs, target_size):\n",
    "    # Convert observation to PIL Image\n",
    "    img = array_to_img(obs)\n",
    "    # Resize image\n",
    "    img = img.resize(target_size)\n",
    "    # Convert to array and normalize\n",
    "    img = img_to_array(img) / 255.0\n",
    "    return img\n",
    "\n",
    "# Define predefined actions (Continuous)\n",
    "predefined_actions = {\n",
    "    0: np.array([0.0, 0.0, 0.0], dtype=np.float32),  # No Action\n",
    "    1: np.array([-1.0, 0.0, 0.0], dtype=np.float32), # Steer Left\n",
    "    2: np.array([1.0, 0.0, 0.0], dtype=np.float32),  # Steer Right\n",
    "    3: np.array([0.0, 1.0, 0.0], dtype=np.float32),  # Accelerate (Gas)\n",
    "    4: np.array([0.0, 0.0, 1.0], dtype=np.float32),  # Brake\n",
    "    # Add more actions as needed\n",
    "}\n",
    "\n",
    "# Load the final model\n",
    "agent.load_model(save_path)\n",
    "\n",
    "# Run the agent in the environment and record a video\n",
    "\n",
    "# Initialize the environment without 'continuous' parameter\n",
    "env_arguments = {\n",
    "    'domain_randomize': False,\n",
    "    'render_mode': 'rgb_array'\n",
    "}\n",
    "\n",
    "env_name = 'CarRacing-v3'\n",
    "\n",
    "env = gym.make(env_name, **env_arguments)\n",
    "\n",
    "# Wrap the environment to record videos\n",
    "video_dir = 'video_recordings'  # Specify the directory to save video recordings\n",
    "env = RecordVideo(env, video_dir)\n",
    "\n",
    "print(\"Environment:\", env_name)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "\n",
    "# Play the game using the trained model\n",
    "play(env, agent, predefined_actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
