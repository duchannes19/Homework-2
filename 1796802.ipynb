{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded.\n"
     ]
    }
   ],
   "source": [
    "#Check if files exists in data folder\n",
    "if os.path.exists('data/'):\n",
    "    print('Files already downloaded.')\n",
    "\n",
    "else:\n",
    "    output_path = 'data.zip'\n",
    "    file_id = '1KDN-rFCq9IDJ7_kNW5y5Co100KNpklz-'\n",
    "    url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    # Download the zip file\n",
    "    gdown.download(url, output_path, quiet=False)\n",
    "\n",
    "    # Extract the contents of the zip file\n",
    "    with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "\n",
    "    # Remove the zip file\n",
    "    os.remove(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already extracted\n"
     ]
    }
   ],
   "source": [
    "if(os.path.exists('test/') and os.path.exists('train')):\n",
    "    print('Files already extracted')\n",
    "else:\n",
    "    print('Extracting the test.zip and train.zip files...')\n",
    "    # Extract the test.zip file\n",
    "    with zipfile.ZipFile('data/public/test.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "\n",
    "    # Extract the train.zip file\n",
    "    with zipfile.ZipFile('data/public/train.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Approach\n",
    "\n",
    "For the first approach, we will use a different architecture to train our model. We will use a custom convolutional neural network (CNN) architecture.\n",
    "\n",
    "### Step 1: Data Loading and Preprocessing\n",
    "\n",
    "Similar to the first approach, we will load and preprocess our dataset using the same transformations. We will also create data loaders for the training and validation sets.\n",
    "\n",
    "### Step 2: Model Architecture\n",
    "\n",
    "In this approach, we will define a custom CNN model. The model will consist of multiple convolutional layers followed by fully connected layers. We will use ReLU activation functions and dropout regularization to prevent overfitting.\n",
    "\n",
    "### Step 3: Training Loop\n",
    "\n",
    "We will train the model using a similar training loop as in the first approach. We will iterate over the training set, compute the loss, perform backpropagation, and update the model's weights.\n",
    "\n",
    "### Step 4: Model Evaluation\n",
    "\n",
    "After training, we will evaluate the model on the validation set. We will calculate the accuracy of the model by comparing the predicted labels with the ground truth labels.\n",
    "\n",
    "### Step 5: Save the Model\n",
    "\n",
    "Finally, we will save the trained model to a file for future use.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this second approach, we used a custom CNN architecture to train our model. This approach allows us to have more control over the model's architecture and potentially achieve better performance. However, it requires more manual design and experimentation compared to using a pre-trained model like ResNet18.\n",
    "\n",
    "It is important to note that the choice of architecture depends on the specific problem and dataset. It is recommended to experiment with different architectures and hyperparameters to find the best model for your task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\andri\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Found 5096 images belonging to 5 classes.\n",
      "Found 548 images belonging to 5 classes.\n",
      "Image input (96, 96, 3)\n",
      "Classes: ['0', '1', '2', '3', '4']\n",
      "Loaded 5096 training samples from  5 classes.\n",
      "Loaded 548 test samples from 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "trainingset = 'train/'\n",
    "validationset = 'test/'\n",
    "\n",
    "batch_size = 64\n",
    "input_shape = ()\n",
    "train_datagen = ImageDataGenerator(\n",
    "    horizontal_flip=False,\\\n",
    "    vertical_flip=False,\n",
    "    validation_split=0.2\n",
    "    )\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=trainingset,\n",
    "    target_size=(96, 96),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    directory=validationset, # same directory as training data\n",
    "    target_size=(96, 96),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data\n",
    "\n",
    "num_samples = train_generator.n\n",
    "num_classes = train_generator.num_classes\n",
    "input_shape = train_generator.image_shape\n",
    "\n",
    "classnames = [k for k,v in train_generator.class_indices.items()]\n",
    "img_h=input_shape[0]\n",
    "img_w=input_shape[1]\n",
    "print(\"Image input %s\" %str(input_shape))\n",
    "print(\"Classes: %r\" %classnames)\n",
    "print('Loaded %d training samples from  %d classes.' %(num_samples,num_classes))\n",
    "print('Loaded %d test samples from %d classes.' %(validation_generator.n,validation_generator.num_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\andri\\miniconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\andri\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Model: \"MyOptimizedCNN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 92, 92, 32)        2432      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 92, 92, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 92, 92, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 88, 88, 64)        51264     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 88, 88, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 88, 88, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 44, 44, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 42, 42, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 42, 42, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 42, 42, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 21, 21, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 56448)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               14450944  \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14614469 (55.75 MB)\n",
      "Trainable params: 14613253 (55.75 MB)\n",
      "Non-trainable params: 1216 (4.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,\\\n",
    "                         Conv2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "\n",
    "def MyCNN(input_shape, num_classes):\n",
    "    model = Sequential(name=\"MyOptimizedCNN\")\n",
    "\n",
    "    # C1 Convolutional Layer \n",
    "    model.add(Conv2D(filters=32, input_shape=input_shape, kernel_size=(5, 5)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # C2 Convolutional Layer\n",
    "    model.add(Conv2D(filters=64, kernel_size=(5, 5)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # C3 Convolutional Layer\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    # Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # D1 Dense Layer\n",
    "    model.add(Dense(256, kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    # Dropout\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # D2 Dense Layer\n",
    "    model.add(Dense(128, kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    # Dropout\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile\n",
    "    optimizer = optimizers.RMSprop(learning_rate=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# create the model\n",
    "model = MyCNN(input_shape, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\andri\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\andri\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "80/80 [==============================] - 57s 694ms/step - loss: 5.0622 - accuracy: 0.4757 - val_loss: 3.1561 - val_accuracy: 0.7190 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "80/80 [==============================] - 54s 679ms/step - loss: 3.0494 - accuracy: 0.5204 - val_loss: 3.3302 - val_accuracy: 0.4909 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "80/80 [==============================] - 55s 691ms/step - loss: 2.5946 - accuracy: 0.5426 - val_loss: 2.1134 - val_accuracy: 0.7117 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "80/80 [==============================] - 55s 688ms/step - loss: 2.2825 - accuracy: 0.5771 - val_loss: 2.0865 - val_accuracy: 0.5949 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "80/80 [==============================] - 56s 696ms/step - loss: 2.0473 - accuracy: 0.5950 - val_loss: 1.8484 - val_accuracy: 0.7172 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "80/80 [==============================] - 56s 694ms/step - loss: 1.8749 - accuracy: 0.6026 - val_loss: 1.8073 - val_accuracy: 0.6423 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "80/80 [==============================] - 55s 687ms/step - loss: 1.7375 - accuracy: 0.6252 - val_loss: 1.7832 - val_accuracy: 0.5803 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "80/80 [==============================] - 55s 684ms/step - loss: 1.6370 - accuracy: 0.6380 - val_loss: 3.2038 - val_accuracy: 0.1953 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "80/80 [==============================] - 54s 677ms/step - loss: 1.5795 - accuracy: 0.6464 - val_loss: 3.6893 - val_accuracy: 0.1259 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "80/80 [==============================] - 54s 675ms/step - loss: 1.4957 - accuracy: 0.6682 - val_loss: 2.5366 - val_accuracy: 0.2536 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "80/80 [==============================] - 54s 672ms/step - loss: 1.2145 - accuracy: 0.7312 - val_loss: 2.7369 - val_accuracy: 0.2299 - lr: 2.0000e-04\n",
      "Epoch 12/50\n",
      "80/80 [==============================] - 54s 675ms/step - loss: 1.0420 - accuracy: 0.7657 - val_loss: 1.9924 - val_accuracy: 0.2464 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Assuming you have a `train_generator` and `validation_generator` defined\n",
    "\n",
    "# Calculate steps per epoch and validation steps\n",
    "steps_per_epoch = len(train_generator)\n",
    "val_steps = len(validation_generator)\n",
    "\n",
    "try:\n",
    "    # Train the model with better training parameters\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=50,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=[early_stopping, reduce_lr]\n",
    "    )\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_8044\\989374102.py:2: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  loss, acc = model.evaluate_generator(validation_generator,steps=val_steps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.783207\n",
      "Test accuracy: 0.580292\n"
     ]
    }
   ],
   "source": [
    "val_steps=validation_generator.n//validation_generator.batch_size+1\n",
    "loss, acc = model.evaluate_generator(validation_generator,steps=val_steps)\n",
    "print('Test loss: %f' %loss)\n",
    "print('Test accuracy: %f' %acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_8044\\1091510445.py:5: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  preds = model.predict_generator(validation_generator,steps=val_steps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.072     0.269     0.114        26\n",
      "           1      0.356     0.291     0.320        55\n",
      "           2      0.493     0.407     0.446        81\n",
      "           3      0.773     0.691     0.730       379\n",
      "           4      0.000     0.000     0.000         7\n",
      "\n",
      "    accuracy                          0.580       548\n",
      "   macro avg      0.339     0.332     0.322       548\n",
      "weighted avg      0.646     0.580     0.608       548\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andri\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\andri\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\andri\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    " \n",
    "preds = model.predict_generator(validation_generator,steps=val_steps)\n",
    "\n",
    "Ypred = np.argmax(preds, axis=1)\n",
    "Ytest = validation_generator.classes  # shuffle=False in test_generator\n",
    "\n",
    "print(classification_report(Ytest, Ypred, labels=None, target_names=classnames, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andri\\AppData\\Local\\Temp\\ipykernel_8044\\548864064.py:3: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  preds = model.predict_generator(validation_generator,verbose=1,steps=val_steps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 2s 158ms/step\n",
      "True                 Predicted         \terrors \terr % \n",
      "------------------------------------------------------------------\n",
      "3                ->  0                 \t62 \t11.31 % \n",
      "2                ->  3                 \t35 \t6.39 % \n",
      "3                ->  2                 \t28 \t5.11 % \n",
      "3                ->  1                 \t27 \t4.93 % \n",
      "1                ->  3                 \t20 \t3.65 % \n",
      "0                ->  3                 \t17 \t3.10 % \n",
      "1                ->  0                 \t15 \t2.74 % \n",
      "2                ->  0                 \t12 \t2.19 % \n",
      "4                ->  3                 \t5 \t0.91 % \n",
      "1                ->  2                 \t4 \t0.73 % \n",
      "0                ->  1                 \t1 \t0.18 % \n",
      "0                ->  2                 \t1 \t0.18 % \n",
      "2                ->  1                 \t1 \t0.18 % \n",
      "4                ->  0                 \t1 \t0.18 % \n",
      "4                ->  2                 \t1 \t0.18 % \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "preds = model.predict_generator(validation_generator,verbose=1,steps=val_steps)\n",
    "\n",
    "Ypred = np.argmax(preds, axis=1)\n",
    "Ytest = validation_generator.classes  # shuffle=False in test_generator\n",
    "\n",
    "cm = confusion_matrix(Ytest, Ypred)\n",
    "\n",
    "conf = [] # data structure for confusions: list of (i,j,cm[i][j])\n",
    "for i in range(0,cm.shape[0]):\n",
    "  for j in range(0,cm.shape[1]):\n",
    "    if (i!=j and cm[i][j]>0):\n",
    "      conf.append([i,j,cm[i][j]])\n",
    "\n",
    "col=2\n",
    "conf = np.array(conf)\n",
    "conf = conf[np.argsort(-conf[:,col])]  # decreasing order by 3-rd column (i.e., cm[i][j])\n",
    "\n",
    "print('%-16s     %-16s  \\t%s \\t%s ' %('True','Predicted','errors','err %'))\n",
    "print('------------------------------------------------------------------')\n",
    "for k in conf:\n",
    "  print('%-16s ->  %-16s  \\t%d \\t%.2f %% ' %(classnames[k[0]],classnames[k[1]],k[2],k[2]*100.0/validation_generator.n))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: CarRacing-v2\n",
      "Action space: Discrete(5)\n",
      "Observation space: Box(0, 255, (96, 96, 3), uint8)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ModuleNotFoundError:\n",
    "    print('gymnasium module not found. Try to install with')\n",
    "    print('pip install gymnasium[box2d]')\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "def play(env, model):\n",
    "\n",
    "    seed = 2000\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    \n",
    "    # drop initial frames\n",
    "    action0 = 0\n",
    "    for i in range(50):\n",
    "        obs,_,_,_,_ = env.step(action0)\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        p = model(np.expand_dims(obs, axis=0)) # reshape input data to have a batch dimension of size 1\n",
    "        action = np.argmax(p)  # adapt to your model\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env_arguments = {\n",
    "    'domain_randomize': False,\n",
    "    'continuous': False,\n",
    "    'render_mode': 'human'\n",
    "}\n",
    "\n",
    "env_name = 'CarRacing-v2'\n",
    "env = gym.make(env_name, **env_arguments)\n",
    "\n",
    "print(\"Environment:\", env_name)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "\n",
    "play(env, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Approach: Deep Reinforcement Learning (DRL)\n",
    "\n",
    "In this approach, we will utilize Deep Reinforcement Learning (DRL) techniques to solve our problem. DRL combines the power of deep neural networks with reinforcement learning algorithms to learn optimal policies in complex environments.\n",
    "\n",
    "### Step 1: Environment Setup\n",
    "\n",
    "First, we need to define our environment. This includes selecting an appropriate gym environment or creating a custom environment that suits our problem. The environment should provide observations, actions, and rewards.\n",
    "\n",
    "### Step 2: Agent Design\n",
    "\n",
    "Next, we design our DRL agent. The agent consists of a deep neural network, often referred to as the Q-network, which takes observations as input and outputs action values for each possible action. We can use popular deep learning frameworks like PyTorch or TensorFlow to implement the Q-network.\n",
    "\n",
    "### Step 3: Training Loop\n",
    "\n",
    "The training loop involves the following steps:\n",
    "\n",
    "1. Initialize the Q-network with random weights.\n",
    "2. Observe the current state from the environment.\n",
    "3. Select an action using an exploration-exploitation strategy, such as epsilon-greedy or softmax.\n",
    "4. Execute the selected action in the environment and observe the next state and reward.\n",
    "5. Update the Q-network using the observed state, action, next state, and reward.\n",
    "6. Repeat steps 2-5 until convergence or a maximum number of iterations.\n",
    "\n",
    "During training, we can use techniques like experience replay and target networks to stabilize and improve the learning process.\n",
    "\n",
    "### Step 4: Evaluation\n",
    "\n",
    "After training, we evaluate the performance of our agent by running it in the environment and measuring its performance metrics, such as average reward or success rate. This helps us assess the effectiveness of our DRL approach.\n",
    "\n",
    "### Step 5: Fine-tuning and Optimization\n",
    "\n",
    "Based on the evaluation results, we can fine-tune and optimize our DRL approach. This may involve adjusting hyperparameters, modifying the network architecture, or trying different exploration-exploitation strategies.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Deep Reinforcement Learning (DRL) offers a powerful approach to solving complex problems by combining deep neural networks with reinforcement learning algorithms. By following the steps outlined above, we can develop and train a DRL agent to learn optimal policies in our environment. However, it is important to note that DRL can be computationally intensive and may require significant computational resources and time for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_frames, n_actions, h_dimension):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # CNN\n",
    "        self.layers_cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_frames, 6, kernel_size=(7, 7), stride=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "            nn.Conv2d(6, 12, kernel_size=(4, 4)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(432, h_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dimension, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.layers_cnn(x)  # (BS, ACTIONS)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                 action_space,\n",
    "                 epsilon=1.0,\n",
    "                 gamma=0.95,\n",
    "                 epsilon_min=0.1,\n",
    "                 epsilon_decay=0.9999,\n",
    "                 lr=1e-3,\n",
    "                 memory_len=5000,\n",
    "                 frames=3,\n",
    "                 hidden_dimension=None,\n",
    "                 device=None):\n",
    "\n",
    "        self.device = device\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory_len = memory_len\n",
    "        self.lr = lr\n",
    "        self.memory = deque(maxlen=self.memory_len)\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.target_model = DQN(frames, len(self.action_space), hidden_dimension).to(self.device)\n",
    "        self.model =        DQN(frames, len(self.action_space), hidden_dimension).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def is_explore(self):\n",
    "        flip = np.random.rand() <= self.epsilon\n",
    "        return flip\n",
    "\n",
    "    def act(self, state, is_only_random=False, is_only_exploit=False):\n",
    "        if not is_only_exploit and self.is_explore() or is_only_random:\n",
    "            action_index = np.random.randint(len(self.action_space))\n",
    "            # print(action_index, self.ACTION_SPACE[action_index])\n",
    "        else:\n",
    "            q_values = self.target_model(state)[0]\n",
    "            action_index = torch.argmax(q_values)\n",
    "            # print(\"predicted action\", action_index)\n",
    "        return self.action_space[action_index]\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, self.action_space.index(action), reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        train_state = []\n",
    "        train_target = []\n",
    "\n",
    "        for state, action_index, reward, next_state, done in minibatch:\n",
    "            # state = torch.Tensor(state)\n",
    "            target = self.model(state)[0]\n",
    "            train_state.append(target)\n",
    "\n",
    "            target_copy = target.detach().clone().to(self.device)\n",
    "            if done:\n",
    "                target_copy[action_index] = reward\n",
    "            else:\n",
    "                t = self.target_model(next_state)[0]\n",
    "                target_copy[action_index] = reward + self.gamma * torch.max(t)\n",
    "            train_target.append(target_copy)\n",
    "\n",
    "        # Actual training\n",
    "        criterion = nn.MSELoss()\n",
    "        pred, tru = torch.stack(train_state), torch.stack(train_target)\n",
    "        loss = criterion(pred, tru)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load_model(self, name):\n",
    "        self.model = torch.load(name)\n",
    "        self.target_model = torch.load(name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def save_model(self, name):\n",
    "        torch.save(self.target_model, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "    SEED = 1\n",
    "\n",
    "    STARTING_EPISODE_TRAIN = 0\n",
    "    ENDING_EPISODE_TRAIN = STARTING_EPISODE_TRAIN + 1000\n",
    "\n",
    "    STARTING_EPISODE_TEST = ENDING_EPISODE_TRAIN + 1\n",
    "    ENDING_EPISODE_TEST = STARTING_EPISODE_TEST + 100\n",
    "\n",
    "    SKIP_FRAMES = 2\n",
    "    TRAINING_BATCH_SIZE = 64\n",
    "    UPDATE_TARGET_MODEL_FREQUENCY = 5\n",
    "    N_FRAMES = 3\n",
    "    HIDDEN_DIMENSION_FC = 150\n",
    "\n",
    "    GAS_WEIGHT = 1.3\n",
    "\n",
    "    ACTION_SPACE = [\n",
    "        (-1, 1, 0.2), (0, 1, 0.2), (1, 1, 0.2),  # .  Action Space Structure\n",
    "        (-1, 1, 0), (0, 1, 0), (1, 1, 0),        # (Steering Wheel, Gas, Break)\n",
    "        (-1, 0, 0.2), (0, 0, 0.2), (1, 0, 0.2),  # .  -1~1     0~1        0~1\n",
    "        (-1, 0, 0), (0, 0, 0), (1, 0, 0)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def write_json_to_file(data, file_path):\n",
    "    \"\"\"\n",
    "    Write JSON data to a file.\n",
    "\n",
    "    Parameters:\n",
    "    - data: A dictionary representing the JSON data.\n",
    "    - file_path: The path where the JSON file will be written.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(data, json_file, indent=4)\n",
    "        print(f\"JSON data successfully written to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing JSON data to {file_path}: {e}\")\n",
    "\n",
    "\n",
    "def read_json_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Read JSON data from a file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: The path of the JSON file to be read.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary representing the JSON data.\n",
    "    - If there is an error reading the file, returns None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        print(f\"JSON data successfully read from {file_path}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading JSON data from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def make_all_paths(is_dynamic_root=True, dir_name=\"rl_class\"):\n",
    "    ROOT = \"data\"\n",
    "\n",
    "    if is_dynamic_root:\n",
    "        date_str = datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "        dir_name = \"rl_class_{}\".format(date_str)\n",
    "    else:\n",
    "        dir_name = dir_name\n",
    "\n",
    "    path_root = ROOT + \"/\" + dir_name + \"/\"\n",
    "    dirs = [\"models\", \"plots\", \"videos\"]\n",
    "    for d in dirs:\n",
    "        path = path_root + d\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        print(\">> Created dir\", path)\n",
    "    return path_root\n",
    "\n",
    "\n",
    "def plot_state_car(data, title=None):\n",
    "    assert len(data.shape) == 3, \"Can only handle 3D mats.\"\n",
    "    assert data.shape[0] < 10, \"Too many states to plot. Adjust the plots position first.\"\n",
    "\n",
    "    # Create a figure with three subplots\n",
    "    fig, axs = plt.subplots(1, data.shape[0], figsize=(10, 4))\n",
    "\n",
    "    # Plot each image using imshow()\n",
    "    for i in range(data.shape[0]):\n",
    "        axs[i].imshow(data[i], cmap='gray')  # You can adjust the colormap if needed\n",
    "        axs[i].axis('off')                   # Turn off axis labels\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_frame_car(data, title=None):\n",
    "    plt.imshow(data, cmap=\"gray\")  # You can adjust the colormap if needed\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def preprocess_frame_car(frame):\n",
    "    def crop(frame):\n",
    "        # Crop to 84x84\n",
    "        return frame[:-12, 6:-6]\n",
    "\n",
    "    def make_img_gray(frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        return frame\n",
    "\n",
    "    def normalize(frame):\n",
    "        return frame / 255.0\n",
    "\n",
    "    # frame = crop(frame)\n",
    "    frame = make_img_gray(frame)\n",
    "    frame = frame.astype(float)\n",
    "    frame = normalize(frame)\n",
    "    # frame = frame * 2 - 1   # maps [0,1] to [-1,1]\n",
    "    return frame\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    # Set seed for Python random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Set seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # disable if deterministic mode is desired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import cv2   # open cv\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "\n",
    "def train_car_racing():\n",
    "    seed_everything(seed=Config.SEED)\n",
    "    PATH_ROOT = make_all_paths(is_dynamic_root=True)\n",
    "    write_json_to_file(dict(Config.__dict__), file_path=PATH_ROOT + \"config.json\")\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('>> Using device:', device)\n",
    "\n",
    "    agent = DQNAgent(frames=Config.N_FRAMES, action_space=Config.ACTION_SPACE, device=device,\n",
    "                     hidden_dimension=Config.HIDDEN_DIMENSION_FC)\n",
    "\n",
    "    # https://www.gymlibrary.dev/environments/box2d/car_racing/\n",
    "    env = gym.make('CarRacing-v2', render_mode=\"rgb_array\")  # , render_mode='human')\n",
    "    env = RecordVideo(env, PATH_ROOT + 'videos', episode_trigger=lambda x: x % Config.UPDATE_TARGET_MODEL_FREQUENCY == 0)\n",
    "\n",
    "    epi_total_rewards = []\n",
    "    for e in range(Config.STARTING_EPISODE_TRAIN, Config.ENDING_EPISODE_TRAIN + 1):\n",
    "        env.episode_id = e\n",
    "\n",
    "        epi_total_reward = 0\n",
    "        epi_negative_reward_counter = 0\n",
    "        epi_time_frame_counter = 1\n",
    "        epi_done = False\n",
    "\n",
    "        init_state = env.reset(seed=e)[0]  # 96, 96, 3 pixels image RGB\n",
    "        init_state = preprocess_frame_car(init_state)  # 96, 96 pixels image GRAY\n",
    "\n",
    "        # (1) EVALUATE STATE: S\n",
    "        state_queue = deque([init_state] * Config.N_FRAMES, maxlen=Config.N_FRAMES)\n",
    "        # plot_state_car(np.array(state_queue))  # visualize S0\n",
    "\n",
    "        while True:\n",
    "            state_tensor = torch.Tensor(np.array(state_queue)).unsqueeze(0).to(device)\n",
    "            action = agent.act(state_tensor)\n",
    "\n",
    "            # (2) EXECUTE ACTION (for several steps)\n",
    "            # (3) EVALUATE S' STATE, REWARD\n",
    "            reward = 0\n",
    "            for _ in range(Config.SKIP_FRAMES):\n",
    "                # execute action\n",
    "                next_state, r, epi_done, _, _ = env.step(action)\n",
    "                # plot_frame_car(next_state)\n",
    "                reward += r\n",
    "                if epi_done:\n",
    "                    break\n",
    "\n",
    "            # (4) ADJUST REWARD\n",
    "            # if getting negative reward 10 times after the tolerance steps, terminate this episode\n",
    "            if epi_time_frame_counter > 100 and reward < 0:\n",
    "                epi_negative_reward_counter += 1\n",
    "            else:\n",
    "                epi_negative_reward_counter = 0\n",
    "\n",
    "            # extra bonus for the model if it uses full gas\n",
    "            if action[1] == 1 and action[2] == 0:\n",
    "                reward *= Config.GAS_WEIGHT\n",
    "\n",
    "            epi_total_reward += reward\n",
    "\n",
    "            # plot_state_car(np.array(state_queue), title=\"STATE 0\")\n",
    "            # process state S'\n",
    "            next_state = preprocess_frame_car(next_state)\n",
    "            next_state_queue = deque([frame for frame in state_queue], maxlen=Config.N_FRAMES)\n",
    "            next_state_queue.append(next_state)\n",
    "            # plot_state_car(np.array(next_state_queue), title=\"STATE 1\")\n",
    "\n",
    "            next_state_tensor = torch.Tensor(np.array(next_state_queue)).unsqueeze(0).to(device)\n",
    "\n",
    "            # (5) STORE OBSERVATIONS\n",
    "            # Memorizing saving state, action reward tuples\n",
    "            agent.memorize(state_tensor, action, reward, next_state_tensor, epi_done)\n",
    "\n",
    "            # S = S'\n",
    "            state_queue = next_state_queue\n",
    "\n",
    "            # early stop if the number of\n",
    "            if epi_negative_reward_counter >= 25 or epi_total_reward < 0:\n",
    "                break\n",
    "\n",
    "            # (6) TRAIN ON BATCHES OF OBSERVATIONS\n",
    "            # train the model with tuple, if there are enough tuples\n",
    "            if len(agent.memory) > Config.TRAINING_BATCH_SIZE:\n",
    "                agent.replay(Config.TRAINING_BATCH_SIZE)\n",
    "\n",
    "            epi_time_frame_counter += 1\n",
    "        epi_total_rewards += [epi_total_reward]\n",
    "\n",
    "        # >>> ON EPISODE END\n",
    "        # print stats\n",
    "        stats_string = 'Episode: {}/{}, Scores(Time Frames): {}, Total Rewards: {:.2}, Epsilon: {:.2}'\n",
    "        print(stats_string.format(\n",
    "            e,\n",
    "            Config.ENDING_EPISODE_TRAIN,\n",
    "            epi_time_frame_counter,\n",
    "            float(epi_total_reward),\n",
    "            float(agent.epsilon))\n",
    "        )\n",
    "\n",
    "        if e % Config.UPDATE_TARGET_MODEL_FREQUENCY == 0:\n",
    "            # plot rewards stats\n",
    "            plt.plot(epi_total_rewards, label=\"cum rew\", color=\"blue\")\n",
    "            plt.title(\"Rewards during episode episode\")\n",
    "            plt.savefig(PATH_ROOT + 'plots/reward_{}.pdf'.format(e))\n",
    "\n",
    "            # save model frequently\n",
    "            agent.save_model(PATH_ROOT + 'models/trial_{}.h5'.format(e))\n",
    "\n",
    "            # swap model\n",
    "            agent.update_target_model()\n",
    "            write_json_to_file({\"CUM_REW\": epi_total_rewards}, PATH_ROOT + \"/stats.json\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "#train_car_racing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
